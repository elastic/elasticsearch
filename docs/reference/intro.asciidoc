[[elasticsearch-intro]]
== Getting started with Elasticsearch
++++
<titleabbrev>Getting started</titleabbrev>
++++

This guide covers the essential basics for getting started with {es}.

You can use it in two ways:

* As a concrete guide for getting hands-on with {es} quickly.
* As a map of the most important {es} information for your use case across the Elastic documentation, Github repos, and blog pages.

This guide covers:

* <<elasticsearch-intro-what-is-es>> Learn about {es} and some of its main use cases.
* <<elasticsearch-intro-deploy>>. Understand your options for deploying {es} in different environments.
* <<documents-indices>>. Understand {es}'s most important primitives and how it stores data.
* <<es-ingestion-overview>>. Understand your options for ingesting data into {es}.
* <<search-analyze>>. Understand your options for searching and analyzing data in {es}.
* <<scalability>>. Learn the essentials about moving your {es} deployment to production.

[[elasticsearch-intro-what-is-es]]
=== What is {es}?

{es-repo}[{es}] is a distributed search and analytics engine, scalable data store, and vector database built on Apache Lucene.
It's optimized for speed and relevance on production-scale workloads.
Use {es} to search, index, store, and analyze data of all shapes and sizes in near real time.

[TIP]
====
{es} has a lot of features. Explore the full list on the https://www.elastic.co/elasticsearch/features[product webpage^].
====

{es} is the heart of the {estc-welcome-current}/stack-components.html[Elastic Stack] and powers the Elastic https://www.elastic.co/enterprise-search[Search], https://www.elastic.co/observability[Observability] and https://www.elastic.co/security[Security] solutions.

{es} is used for a wide and growing range of use cases. Here are a few examples:

* *Monitor log and event data*. Store logs, metrics, and event data for observability and security information and event management (SIEM).
* *Build search applications*. Add search capabilities to apps or websites, or build enterprise search engines over your organization's internal data sources.
* *Vector database*. Store and search vectorized data, and create vector embeddings with built-in and third-party natural language processing (NLP) models.
* *Retrieval augmented generation (RAG)*. Use {es} as a retrieval engine to augment Generative AI models.
* *Application and security monitoring*. Monitor and analyze application performance and security data effectively.

This is just a sample of search, observability, and security use cases enabled by {es}. 
Refer to our https://www.elastic.co/customers/success-stories[customer success stories] for concrete examples across a range of industries.

[discrete]
[[elasticsearch-intro-elastic-stack]]
.What is the Elastic Stack?
*******************************
{es} is the core component of the Elastic Stack, a suite of products for collecting, storing, searching, and visualizing data.
https://www.elastic.co/guide/en/starting-with-the-elasticsearch-platform-and-its-solutions/current/stack-components.html[Learn more about the Elastic Stack].
*******************************

[[elasticsearch-intro-deploy]]
=== Deployment options

To use {es}, you need a running instance of the {es} service.
You can deploy {es} in various ways.

**Quick start options**

* <<run-elasticsearch-locally,*Local dev*>>. Get started quickly with a minimal local Docker setup. Recommended if you like getting started in a local environment.
* {cloud}/ec-getting-started-trial.html[*Elastic Cloud*]. {es} is available as part of our hosted Elastic Stack offering, deployed in the cloud with your provider of choice. Sign up for a https://cloud.elastic.co/registration[14 day free trial].
* {serverless-docs}/general/sign-up-trial[*Elastic Cloud Serverless* (technical preview)]. Create serverless projects for autoscaled and fully managed {es} deployments. Sign up for a https://cloud.elastic.co/serverless-registration[14 day free trial].

**Advanced deployment options**

* <<elasticsearch-deployment-options,*Self-managed*>>. Install, configure, and run {es} on your own premises.
* {ece-ref}/Elastic-Cloud-Enterprise-overview.html[*Elastic Cloud Enterprise*]. Deploy Elastic Cloud on public or private clouds, virtual machines, or your own premises.
* {eck-ref}/k8s-overview.html[*Elastic Cloud on Kubernetes*]. Deploy Elastic Cloud on Kubernetes.

[discrete]
[[elasticsearch-intro-cluster-nodes-shards]]
=== Clusters, nodes, and shards

A single instance of Elasticsearch is a node.
A collection of connected nodes is called a cluster.
In production environments, nodes typically run on individual servers or virtual machines.
Adding nodes to a cluster increases capacity and redundancy.

{es} automatically distributes data and query load across all available nodes using shards and replicas.
Shards are the internal building blocks of an index, and each shard is a single Lucene index.

.What you need to know
*******************************
Nodes, clusters and shards are what make Elasticsearch distributed and scalable.
Learn more in <<scalability,Plan for production>>.

These concepts aren't essential if you're just getting started.
How you <<elasticsearch-intro-deploy,deploy Elasticsearch>> in production determines what you need to know:

* *Self-managed Elasticsearch*. You are responsible for setting up and managing nodes, clusters, shards, and replicas.
This includes managing the underlying infrastructure, scaling, and ensuring high availability through failover and backup strategies.
* *Elastic Cloud*. Elastic can autoscale resources in response to workload changes.
Choose from different deployment types to apply sensible defaults for your use case.
A basic understanding of nodes, shards, and replicas is still important.
* *Elastic Cloud Serverless*. You don't need to know about nodes, shards, replicas.
These resources are 100% automated on the serverless platform, which is designed to scale with your workload.
*******************************

// new html page 
[[documents-indices]]
=== Indices, documents, and fields
++++
<titleabbrev>Indices and documents</titleabbrev>
++++

The index is the fundamental unit of storage in {es}, a logical namespace for storing data that share similar characteristics.
After you have {es} <<elasticsearch-intro-deploy,deployed>>, you'll get started by creating an index to store your data.

[TIP]
====
A closely related concept is a <<data-streams,data stream>>.
This index abstraction is optimized for append-only time-series data, and is made up of hidden, auto-generated backing indices.
If you're working with time-series data, we recommend the {observability-guide}[Elastic Observability] solution.
====

Some key facts about indices:

* An index is a collection of documents
* An index has a unique name
* An index can also be referred to by an <<aliases,alias>>
* An index has a mapping that defines the schema of its documents

[discrete]
[[elasticsearch-intro-documents-fields]]
==== Documents and fields

{es} serializes and stores data in the form of JSON documents.
A document is a set of fields, which are key-value pairs that contain your data.
Each document has a unique ID, which you can create or have {es} auto-generate.

A simple {es} document might look like this:

[source,js]
----
{
  "_index": "my-first-elasticsearch-index",
  "_id": "DyFpo5EBxE8fzbb95DOa",
  "_version": 1,
  "_seq_no": 0,
  "_primary_term": 1,
  "found": true,
  "_source": {
    "email": "john@smith.com",
    "first_name": "John",
    "last_name": "Smith",
    "info": {
      "bio": "Eco-warrior and defender of the weak",
      "age": 25,
      "interests": [
        "dolphins",
        "whales"
      ]
    },
    "join_date": "2024/05/01"
  }
}
----
// NOTCONSOLE

[discrete]
[[elasticsearch-intro-documents-fields-data-metadata]]
==== Data and metadata

An indexed document contains data and metadata.
In {es}, <<mapping-fields,metadata fields>> are prefixed with an underscore.

The most important metadata fields are:

* `_source`. Contains the original JSON document.
* `_index`. The name of the index where the document is stored.
* `_id`. The document's ID. IDs must be unique per index.

[discrete]
[[elasticsearch-intro-documents-fields-mappings]]
==== Mappings and data types

Each index has a <<mapping,mapping>> or schema for how the fields in your documents are indexed.
A mapping defines the <<mapping-types,data type>> for each field, how the field should be indexed,
and how it should be stored.
When adding documents to {es}, you have two options for mappings:

* <<mapping-dynamic, Dynamic mapping>>. Let {es} automatically detect the data types and create the mappings for you. This is great for getting started quickly.
* <<mapping-explicit, Explicit mapping>>. Define the mappings up front by specifying data types for each field. Recommended for production use cases.

[TIP]
====
You can use a combination of dynamic and explicit mapping on the same index.
This is useful when you have a mix of known and unknown fields in your data.
====

// New html page
[[es-ingestion-overview]]
=== Ingestion options

There are multiple ways to ingest data into {es}.

Here are a few options for getting started:

* <<docs,*API*>>. Use the {es} Document APIs to index documents directly, using the Dev Tools Console in {kib} or a programming language client. Once you get past the very basics, you'll want to use the <<docs-bulk,Bulk API>> for efficiency.
* {kibana-ref}/connect-to-elasticsearch.html#upload-data-kibana[*File upload*]. Use the {kib} File Uploader to upload and index CSV, JSON, and log files.
* {kibana-ref}/connect-to-elasticsearch.html#_add_sample_data[*Sample data*]. Load sample data sets into your {es} cluster using {kib}.
* {enterprise-search-ref}/crawler.html[*Web Crawler*]. Extract and index web page content into {es} documents.
* {enterprise-search-ref}/connectors.html[*Connectors*]. Sync data from various third-party data sources to create searchable, read-only replicas in {es}.

[TIP]
====
If you're interested in data ingestion pipelines for time-series data, use the decision tree in the {cloud}/ec-cloud-ingest-data.html#ec-data-ingest-pipeline[Elastic Cloud docs] to understand your options.
====

// New html page
[[search-analyze]]
=== Search and analyze data

You can use {es} as a basic document store to simply retrieve documents and their
metadata.
However, the real power of {es} comes from its advanced search and analytics capabilities.

The primary tool for interacting with {es} today is the <<query-dsl, Query DSL>>.
We'll be using the Query DSL for the examples in this guide.

[TIP]
====
<<esql,{esql}>> is our new piped query language (and compute engine) that is initally mainly focused on time-series data like logs and metrics.
====

[discrete]
[[search-data]]
==== Searching and filtering data

{es} support a wide range of search techniques including:

* <<full-text-queries,*Full-text search*>>. Search text that has been analyzed and indexed to support full-text search based on relevance.
* <<keyword,*Keyword search*>>. Search for exact matches using `keyword` fields.
* <<semantic-search-semantic-text,*Semantic search*>>. Search `semantic_field` fields using dense or sparse vector search on embeddings generated in your {es} cluster.
* <<knn-search,*K-nearest neighbors (kNN) search*>>. Search for similar dense vectors using the kNN algorithm for embeddings generated outside of {es}.
* <<geo-queries,*Geospatial search*>>. Search for locations and calculate spatial relationships using geospatial queries.

Learn about the full range of queries supported by the <<query-dsl,Query DSL>>.

You can also filter data using the Query DSL.
Filters enable you to include or exclude documents by retrieving documents that match specific field-level criteria.
A query that uses the `filter` parameter indicates <<filter-context,filter context>>.

{esql} also has powerful filtering capabilities.

[discrete]
[[intro-search-query-languages]]
==== Query languages

The following languages can be used to query {es}:

[cols="1,2,2,1", options="header"]
|===
| Name | Description | Use cases | API endpoint

| <<query-dsl,Query DSL>>
| Primary query language for {es}. Powerful and flexible JSON-style language that enables complex queries.
| Supports full-text search, semantic search, keyword search, filtering, aggregations, and more.
| <<search-search,`_search`>>


| <<esql,{esql}>>
| Introduced in *8.11*, the Elasticsearch Query Language ({esql}) is a piped query language language for filtering, transforming, and analyzing data.
| Initially tailored towards working with time series data like logs and metrics. 
Robust integration with {kib} for querying, visualizing, and analyzing data.
Does not yet support full-text search.
| <<esql-rest,`_query`>>

| https://www.elastic.co/guide/en/elasticsearch/client/index.html[Programming languages]
| Client libraries and APIs for interacting with Elasticsearch using your programming language of choice. Includes Java, Python, JavaScript, Rust, and more.
| For application development. Enables data indexing, searching, updating, and aggregation directly from your application code.
| N/A

| <<eql,EQL>>
| Event Query Language (EQL) is a query language for event-based time series data. Data must contain an `@timestamp` field to use EQL.
| Designed for the threat hunting security use case.
| <<eql-apis,`_eql`>>

| <<xpack-sql,Elasticsearch SQL>>
| Allows native, real-time SQL-like querying against {es} data. JDBC and ODBC drivers are available for integration with business intelligence (BI) tools.
| Enables users familiar with SQL to query {es} data using familiar syntax for BI and reporting.
| <<sql-apis,`_sql`>>

|===

[discrete]
[[analyze-data]]
==== Analyzing your data

{es} enables a host of use cases based on aggregations and analytics.

[discrete]
[[analyze-data-query-dsl]]
===== Query DSL

<<search-aggregations,Aggregations>> are the primary tool for analyzing {es} data using the Query DSL.
An aggregation summarizes your data as metrics, statistics, or other analytics:

* <<search-aggregations-metrics,Metric>>. Calculate metrics,
such as a sum or average, from field values.
* <<search-aggregations-bucket,Bucket>>. Group documents into buckets based on field values, ranges,
or other criteria.
* <<search-aggregations-pipeline,Pipeline>>. Run aggregations on the results of other aggregations.

Run aggregations by specifying the <<search-search,search API>>'s `aggs` parameter.
Learn more in <<run-an-agg,Run an aggregation>>.

[discrete]
[[analyze-data-esql]]
===== {esql}

<<esql,Elasticsearch Query Language ({esql})>> is a piped query language for filtering, transforming, and analyzing data.
{esql} is built on top of a new compute engine, where search, aggregation, and transformation functions are
directly executed within {es} itself.
It comes with a comprehensive set of <<esql-functions-operators,functions and operators>> for working with data and has robust integration with {kib}'s Discover, dashboards and visualizations.

Learn more in <<esql-getting-started,Getting started with {esql}>>, or try https://www.elastic.co/training/introduction-to-esql[our training course].

[discrete]
[[elasticsearch-intro-search-analyze-next-step]]
==== Next step

Now that you understand the basics of searching and analyzing data in {es}, you're ready to start ingesting data.
In this getting started guide, we'll be using Dev Tools Console syntax to call the {es} REST APIs.

[[elasticsearch-intro-start-rest-api]]
=== Start with the REST API

//TODO:WIP

You send data and other requests to {es} using REST APIs.
This lets you interact
with {es} using any client that sends HTTP requests, such as
https://curl.se[curl] or our https://www.elastic.co/guide/en/elasticsearch/client/index.html[programming language clients].

In this guide we'll use {kib}'s Console to send requests to {es}.

[TIP]
====
Elastic docs default to using Console syntax for code samples, but you can also copy these examples as `curl` and other programming languages, using the code sample language selector.

As of 8.16.0 (and earlier on Elastic Cloud Serverless), you can copy requests directly from the Console UI into different programming language syntaxes, including Python, JavaScript, and Ruby.
====

Test your connection to {es} by sending a request to the root endpoint.

[source,console]
----
GET /
----

This should return a response with information about your {es} cluster.

[[scalability]]
=== Plan for production

{es} is built to be always available and to scale with your needs. It does this
by being distributed by nature. You can add servers (nodes) to a cluster to
increase capacity and {es} automatically distributes your data and query load
across all of the available nodes. No need to overhaul your application, {es}
knows how to balance multi-node clusters to provide scale and high availability.
The more nodes, the merrier.

How does this work? Under the covers, an {es} index is really just a logical
grouping of one or more physical shards, where each shard is actually a
self-contained index. By distributing the documents in an index across multiple
shards, and distributing those shards across multiple nodes, {es} can ensure
redundancy, which both protects against hardware failures and increases
query capacity as nodes are added to a cluster. As the cluster grows (or shrinks),
{es} automatically migrates shards to rebalance the cluster.

There are two types of shards: primaries and replicas. Each document in an index
belongs to one primary shard. A replica shard is a copy of a primary shard.
Replicas provide redundant copies of your data to protect against hardware
failure and increase capacity to serve read requests
like searching or retrieving a document.

The number of primary shards in an index is fixed at the time that an index is
created, but the number of replica shards can be changed at any time, without
interrupting indexing or query operations.

[discrete]
[[it-depends]]
==== Shard size and number of shards

There are a number of performance considerations and trade offs with respect
to shard size and the number of primary shards configured for an index. The more
shards, the more overhead there is simply in maintaining those indices. The
larger the shard size, the longer it takes to move shards around when {es}
needs to rebalance a cluster.

Querying lots of small shards makes the processing per shard faster, but more
queries means more overhead, so querying a smaller
number of larger shards might be faster. In short...it depends.

As a starting point:

* Aim to keep the average shard size between a few GB and a few tens of GB. For
  use cases with time-based data, it is common to see shards in the 20GB to 40GB
  range.

* Avoid the gazillion shards problem. The number of shards a node can hold is
  proportional to the available heap space. As a general rule, the number of
  shards per GB of heap space should be less than 20.

The best way to determine the optimal configuration for your use case is
through https://www.elastic.co/elasticon/conf/2016/sf/quantitative-cluster-sizing[
testing with your own data and queries].

[discrete]
[[disaster-ccr]]
==== Disaster recovery

A cluster's nodes need good, reliable connections to each other. To provide
better connections, you typically co-locate the nodes in the same data center or
nearby data centers. However, to maintain high availability, you
also need to avoid any single point of failure. In the event of a major outage
in one location, servers in another location need to be able to take over. The
answer? {ccr-cap} (CCR).

CCR provides a way to automatically synchronize indices from your primary cluster
to a secondary remote cluster that can serve as a hot backup. If the primary
cluster fails, the secondary cluster can take over. You can also use CCR to
create secondary clusters to serve read requests in geo-proximity to your users.

{ccr-cap} is active-passive. The index on the primary cluster is
the active leader index and handles all write requests. Indices replicated to
secondary clusters are read-only followers.

[discrete]
[[admin]]
==== Security, management, and monitoring

As with any enterprise system, you need tools to secure, manage, and
monitor your {es} clusters. Security, monitoring, and administrative features
that are integrated into {es} enable you to use {kibana-ref}/introduction.html[{kib}]
as a control center for managing a cluster. Features like <<downsampling,
downsampling>> and <<index-lifecycle-management, index lifecycle management>>
help you intelligently manage your data over time.

Refer to <<monitor-elasticsearch-cluster>> for more information.
[role="xpack"]
[[semantic-text]]
=== Semantic text field type
++++
<titleabbrev>Semantic text</titleabbrev>
++++

beta[]

The `semantic_text` field type automatically generates embeddings for text content using an inference endpoint.
Long passages are <<auto-text-chunking, automatically chunked>> to smaller sections to enable the processing of larger corpuses of text.

The `semantic_text` field type specifies an inference endpoint identifier that will be used to generate embeddings.
You can create the inference endpoint by using the <<put-inference-api>>.
This field type and the <<query-dsl-semantic-query,`semantic` query>> type make it simpler to perform semantic search on your data.
If you don't specify an inference endpoint, the <<infer-service-elser,ELSER service>> is used by default.

Using `semantic_text`, you won't need to specify how to generate embeddings for your data, or how to index it.
The {infer} endpoint automatically determines the embedding generation, indexing, and query to use.

If you use the ELSER service, you can set up `semantic_text` with the following API request:

[source,console]
------------------------------------------------------------
PUT my-index-000001
{
  "mappings": {
    "properties": {
      "inference_field": {
        "type": "semantic_text"
      }
    }
  }
}
------------------------------------------------------------

If you use a service other than ELSER, you must create an {infer} endpoint using the <<put-inference-api>> and reference it when setting up `semantic_text` as the following example demonstrates:

[source,console]
------------------------------------------------------------
PUT my-index-000002
{
  "mappings": {
    "properties": {
      "inference_field": {
        "type": "semantic_text",
        "inference_id": "my-openai-endpoint" <1>
      }
    }
  }
}
------------------------------------------------------------
// TEST[skip:Requires inference endpoint]
<1> The `inference_id` of the {infer} endpoint to use to generate embeddings.


The recommended way to use semantic_text is by having dedicated {infer} endpoints for ingestion and search.
This ensures that search speed remains unaffected by ingestion workloads, and vice versa.
After creating dedicated {infer} endpoints for both, you can reference them using the `inference_id` and `search_inference_id` parameters when setting up the index mapping for an index that uses the `semantic_text` field.

[source,console]
------------------------------------------------------------
PUT my-index-000003
{
  "mappings": {
    "properties": {
      "inference_field": {
        "type": "semantic_text",
        "inference_id": "my-elser-endpoint-for-ingest",
        "search_inference_id": "my-elser-endpoint-for-search"
      }
    }
  }
}
------------------------------------------------------------
// TEST[skip:Requires inference endpoint]


[discrete]
[[semantic-text-params]]
==== Parameters for `semantic_text` fields

`inference_id`::
(Required, string)
{infer-cap} endpoint that will be used to generate the embeddings for the field.
This parameter cannot be updated.
Use the <<put-inference-api>> to create the endpoint.
If `search_inference_id` is specified, the {infer} endpoint defined by `inference_id` will only be used at index time.

`search_inference_id`::
(Optional, string)
{infer-cap} endpoint that will be used to generate embeddings at query time.
You can update this parameter by using the <<indices-put-mapping, Update mapping API>>.
Use the <<put-inference-api>> to create the endpoint.
If not specified, the {infer} endpoint defined by `inference_id` will be used at both index and query time.

[discrete]
[[infer-endpoint-validation]]
==== {infer-cap} endpoint validation

The `inference_id` will not be validated when the mapping is created, but when documents are ingested into the index.
When the first document is indexed, the `inference_id` will be used to generate underlying indexing structures for the field.

WARNING: Removing an {infer} endpoint will cause ingestion of documents and semantic queries to fail on indices that define `semantic_text` fields with that {infer} endpoint as their `inference_id`.
Trying to <<delete-inference-api,delete an {infer} endpoint>> that is used on a `semantic_text` field will result in an error.


[discrete]
[[auto-text-chunking]]
==== Text chunking

{infer-cap} endpoints have a limit on the amount of text they can process.
To allow for large amounts of text to be used in semantic search, `semantic_text` automatically generates smaller passages if needed, called _chunks_.

Each chunk will include the text subpassage and the corresponding embedding generated from it.
When querying, the individual passages will be automatically searched for each document, and the most relevant passage will be used to compute a score.

For more details on chunking and how to configure chunking settings, see <<infer-chunking-config, Configuring chunking>> in the Inference API documentation.


[discrete]
[[semantic-text-structure]]
==== `semantic_text` structure

Once a document is ingested, a `semantic_text` field will have the following structure:

[source,console-result]
------------------------------------------------------------
"inference_field": {
  "text": "these are not the droids you're looking for", <1>
  "inference": {
    "inference_id": "my-elser-endpoint", <2>
    "model_settings": { <3>
      "task_type": "sparse_embedding"
    },
    "chunks": [ <4>
      {
        "text": "these are not the droids you're looking for",
        "embeddings": {
          (...)
        }
      }
    ]
  }
}
------------------------------------------------------------
// TEST[skip:TBD]
<1> The field will become an object structure to accommodate both the original
text and the inference results.
<2> The `inference_id` used to generate the embeddings.
<3> Model settings, including the task type and dimensions/similarity if
applicable.
<4> Inference results will be grouped in chunks, each with its corresponding
text and embeddings.

Refer to <<semantic-search-semantic-text,this tutorial>> to learn more about
semantic search using `semantic_text` and the `semantic` query.


[discrete]
[[custom-indexing]]
==== Customizing `semantic_text` indexing

`semantic_text` uses defaults for indexing data based on the {infer} endpoint
specified. It enables you to quickstart your semantic search by providing
automatic {infer} and a dedicated query so you don't need to provide further
details.

In case you want to customize data indexing, use the
<<sparse-vector,`sparse_vector`>> or <<dense-vector,`dense_vector`>> field
types and create an ingest pipeline with an
<<inference-processor, {infer} processor>> to generate the embeddings.
<<semantic-search-inference,This tutorial>> walks you through the process. In
these cases - when you use `sparse_vector` or `dense_vector` field types instead
of the `semantic_text` field type to customize indexing - using the 
<<query-dsl-semantic-query,`semantic_query`>> is not supported for querying the 
field data.


[discrete]
[[update-script]]
==== Updates to `semantic_text` fields

Updates that use scripts are not supported for an index contains a `semantic_text` field.
Even if the script targets non-`semantic_text` fields, the update will fail when the index contains a `semantic_text` field.


[discrete]
[[copy-to-support]]
==== `copy_to` support

The `semantic_text` field type can be the target of
<<copy-to,`copy_to` fields>>. This means you can use a single `semantic_text`
field to collect the values of other fields for semantic search. Each value has
its embeddings calculated separately; each field value is a separate set of chunk(s) in
the resulting embeddings.

This imposes a restriction on bulk requests and ingestion pipelines that update documents with `semantic_text` fields.
In these cases, all fields that are copied to a `semantic_text` field, including the `semantic_text` field value, must have a value to ensure every embedding is calculated correctly.

For example, the following mapping:

[source,console]
------------------------------------------------------------
PUT test-index
{
    "mappings": {
        "properties": {
            "infer_field": {
                "type": "semantic_text",
                "inference_id": "my-elser-endpoint"
            },
            "source_field": {
                "type": "text",
                "copy_to": "infer_field"
            }
        }
    }
}
------------------------------------------------------------
// TEST[skip:TBD]

Will need the following bulk update request to ensure that `infer_field` is updated correctly:

[source,console]
------------------------------------------------------------
PUT test-index/_bulk
{"update": {"_id": "1"}}
{"doc": {"infer_field": "updated inference field", "source_field": "updated source field"}}
------------------------------------------------------------
// TEST[skip:TBD]

Notice that both the `semantic_text` field and the source field are updated in the bulk request.


[discrete]
[[limitations]]
==== Limitations

`semantic_text` field types have the following limitations:

* `semantic_text` fields are not currently supported as elements of <<nested,nested fields>>.
* `semantic_text` fields can't currently be set as part of <<dynamic-templates>>.
* `semantic_text` fields can't be defined as <<multi-fields,multi-fields>> of another field, nor can they contain other fields as multi-fields.

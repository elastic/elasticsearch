[[modules-discovery]]
== Discovery and cluster formation

The discovery and cluster formation module is responsible for discovering
nodes, electing a master, forming a cluster, and publishing the cluster state
each time it changes. It is integrated with other modules, for example, all
communication between nodes is done using the <<modules-transport,transport>>
module.

It is separated into several sections, which are explained below:

*   <<modules-discovery-hosts-providers,Discovery>> is the process where nodes
    find each other when the master is unknown, such as when a node has just
    started up or when the previous master has failed.
*   <<modules-discovery-bootstrap-cluster>> is required when an Elasticsearch
    cluster starts up for the very first time. In <<dev-vs-prod-mode,development
    mode>>, with no discovery settings configured, this is automatically
    performed by the nodes themselves. As this auto-bootstrapping is
    <<modules-discovery-quorums,inherently unsafe>>, running a node in
    <<dev-vs-prod-mode,production mode>> requires bootstrapping to be explicitly
    configured via the `cluster.initial_master_nodes` setting.
*   It is recommended to have a small and fixed number of master-eligible nodes
    in a cluster, and to scale the cluster up and down by adding and removing
    master-ineligible nodes only. However there are situations in which it may
    be desirable to add or remove some master-eligible nodes to or from a
    cluster. A section on <<modules-discovery-adding-removing-nodes,adding and
    removing nodes>> describes this process as well as the extra steps that need
    to be performed when removing more than half of the master-eligible nodes at
    the same time.
*   <<cluster-state-publishing,Cluster state publishing>> covers how a master
    publishes cluster states to the other nodes in the cluster.
*   The <<no-master-block,no-master block>> is put in place when there is no
    known elected master, and can be configured to determine which operations
    should be rejected when it is in place.
*   <<master-election,Master election>> and <<fault-detection,fault detection>>
    sections cover advanced settings to influence the election and fault
    detection processes.
*   <<modules-discovery-quorums,Quorum-based decision making>> explains the
    design behind the master election and auto-reconfiguration logic.

[float]
[[modules-discovery-hosts-providers]]
=== Discovery

The cluster formation module uses a list of _seed_ nodes in order to start off
the discovery process. At startup, or when disconnected from a master,
Elasticsearch tries to connect to each seed node in its list, and holds a
gossip-like conversation with them to find other nodes and to build a complete
picture of the master-eligible nodes in the cluster. By default the cluster
formation module offers two hosts providers to configure the list of seed
nodes: a _settings_-based and a _file_-based hosts provider, but can be
extended to support cloud environments and other forms of hosts providers via
{plugins}/discovery.html[discovery plugins]. Hosts providers are configured
using the `discovery.zen.hosts_provider` setting, which defaults to the
_settings_-based hosts provider. Multiple hosts providers can be specified as a
list.

[float]
[[settings-based-hosts-provider]]
===== Settings-based hosts provider

The settings-based hosts provider use a node setting to configure a static list
of hosts to use as seed nodes. These hosts can be specified as hostnames or IP
addresses; hosts specified as hostnames are resolved to IP addresses during each
round of discovery. Note that if you are in an environment where DNS resolutions
vary with time, you might need to adjust your <<networkaddress-cache-ttl,JVM
security settings>>.

The list of hosts is set using the `discovery.zen.ping.unicast.hosts` static
setting.  This is either an array of hosts or a comma-delimited string. Each
value should be in the form of `host:port` or `host` (where `port` defaults to
the setting `transport.profiles.default.port` falling back to
`transport.tcp.port` if not set). Note that IPv6 hosts must be bracketed. The
default for this setting is `127.0.0.1, [::1]`

[source,yaml]
--------------------------------------------------
discovery.zen.ping.unicast.hosts:
   - 192.168.1.10:9300
   - 192.168.1.11 <1>
   - seeds.mydomain.com <2>
--------------------------------------------------
<1> The port will default to `transport.profiles.default.port` and fallback to
    `transport.tcp.port` if not specified.
<2> A hostname that resolves to multiple IP addresses will try all resolved
    addresses.

Additionally, the `discovery.zen.ping.unicast.hosts.resolve_timeout` configures
the amount of time to wait for DNS lookups on each round of discovery. This is
specified as a <<time-units, time unit>> and defaults to 5s.

Unicast discovery uses the <<modules-transport,transport>> module to perform the
discovery.

[float]
[[file-based-hosts-provider]]
===== File-based hosts provider

The file-based hosts provider configures a list of hosts via an external file.
Elasticsearch reloads this file when it changes, so that the list of seed nodes
can change dynamically without needing to restart each node. For example, this
gives a convenient mechanism for an Elasticsearch instance that is run in a
Docker container to be dynamically supplied with a list of IP addresses to
connect to when those IP addresses may not be known at node startup.

To enable file-based discovery, configure the `file` hosts provider as follows:

[source,txt]
----------------------------------------------------------------
discovery.zen.hosts_provider: file
----------------------------------------------------------------

Then create a file at `$ES_PATH_CONF/unicast_hosts.txt` in the format described
below. Any time a change is made to the `unicast_hosts.txt` file the new changes
will be picked up by Elasticsearch and the new hosts list will be used.

Note that the file-based discovery plugin augments the unicast hosts list in
`elasticsearch.yml`: if there are valid unicast host entries in
`discovery.zen.ping.unicast.hosts` then they will be used in addition to those
supplied in `unicast_hosts.txt`.

The `discovery.zen.ping.unicast.hosts.resolve_timeout` setting also applies to
DNS lookups for nodes specified by address via file-based discovery. This is
specified as a <<time-units, time unit>> and defaults to 5s.

The format of the file is to specify one node entry per line.  Each node entry
consists of the host (host name or IP address) and an optional transport port
number.  If the port number is specified, is must come immediately after the
host (on the same line) separated by a `:`.  If the port number is not
specified, a default value of 9300 is used.

For example, this is an example of `unicast_hosts.txt` for a cluster with four
nodes that participate in unicast discovery, some of which are not running on
the default port:

[source,txt]
----------------------------------------------------------------
10.10.10.5
10.10.10.6:9305
10.10.10.5:10005
# an IPv6 address
[2001:0db8:85a3:0000:0000:8a2e:0370:7334]:9301
----------------------------------------------------------------

Host names are allowed instead of IP addresses (similar to
`discovery.zen.ping.unicast.hosts`), and IPv6 addresses must be specified in
brackets with the port coming after the brackets.

It is also possible to add comments to this file. All comments must appear on
their lines starting with `#` (i.e. comments cannot start in the middle of a
line).

[float]
[[ec2-hosts-provider]]
===== EC2 hosts provider

The {plugins}/discovery-ec2.html[EC2 discovery plugin] adds a hosts provider
that uses the https://github.com/aws/aws-sdk-java[AWS API] to find a list of
seed nodes.

[float]
[[azure-classic-hosts-provider]]
===== Azure Classic hosts provider

The {plugins}/discovery-azure-classic.html[Azure Classic discovery plugin] adds
a hosts provider that uses the Azure Classic API find a list of seed nodes.

[float]
[[gce-hosts-provider]]
===== Google Compute Engine hosts provider

The {plugins}/discovery-gce.html[GCE discovery plugin] adds a hosts provider
that uses the GCE API find a list of seed nodes.

[float]
==== Discovery settings

Discovery operates in two phases: First, each node probes the addresses of all
known master-eligible nodes by connecting to each address and attempting to
identify the node to which it is connected. Secondly it shares with the remote
node a list of all of its known master-eligible peers and the remote node
responds with _its_ peers in turn. The node then probes all the new nodes that
it just discovered, requests their peers, and so on.

If the node is not master-eligible then it continues this discovery process
until it has discovered an elected master node. If no elected master is
discovered then the node will retry after `discovery.find_peers_interval` which
defaults to `1s`.

If the node is master-eligible then it continues this discovery process until it
has either discovered an elected master node or else it has discovered enough
masterless master-eligible nodes to complete an election. If neither of these
occur quickly enough then the node will retry after
`discovery.find_peers_interval` which defaults to `1s`.

The discovery process is controlled by the following settings.

`discovery.find_peers_interval`::

    Sets how long a node will wait before attempting another discovery round.
    Defaults to `1s`.

`discovery.request_peers_timeout`::

    Sets how long a node will wait after asking its peers again before
    considering the request to have failed. Defaults to `3s`.

`discovery.probe.connect_timeout`::

    Sets how long to wait when attempting to connect to each address. Defaults
    to `3s`.

`discovery.probe.handshake_timeout`::

    Sets how long to wait when attempting to identify the remote node via a
    handshake. Defaults to `1s`.

`discovery.cluster_formation_warning_timeout`::

    Sets how long a node will try to form a cluster before logging a warning
    that the cluster did not form. Defaults to `10s`.

If a cluster has not formed after `discovery.cluster_formation_warning_timeout`
has elapsed then the node will log a warning message that starts with the phrase
`master not discovered` which describes the current state of the discovery
process.

[float]
[[modules-discovery-bootstrap-cluster]]
=== Bootstrapping a cluster

Starting an Elasticsearch cluster for the very first time requires the initial
set of master-eligible nodes to be explicitly set on one or more of the
master-eligible nodes in the cluster using this setting:

`cluster.initial_master_nodes`::

    Sets a list of the <<node.name,node names>> or transport addresses of the
    initial set of master-eligible nodes in a brand-new cluster. By default
    this list is empty, meaning that this node expects to join a cluster that
    has already been bootstrapped.

This setting can be given on the command line when starting up each node, or
added to the `elasticsearch.yml` configuration file. Once the cluster has
formed this setting is no longer required and should be removed.

For a cluster with 3 master-eligible nodes (with <<node.name,node names>>
`master-a`, `master-b` and `master-c`) the configuration will look as follows:

[source,yaml]
--------------------------------------------------
cluster.initial_master_nodes:
  - master-a
  - master-b
  - master-c
--------------------------------------------------

Alternatively the IP addresses or hostnames
(<<node.name,if node name defaults to the host name>>) can be used. If there
is more than one Elasticsearch node with the same IP address or hostname then
the transport ports must also be given

[source,yaml]
--------------------------------------------------
cluster.initial_master_nodes:
  - 10.0.10.101
  - 10.0.10.102:9300
  - 10.0.10.102:9301
  - master-node-hostname
--------------------------------------------------

Like all node settings, it is also possible to specify the initial set of
master nodes on the command-line that is used to start Elasticsearch:

[source,bash]
--------------------------------------------------
$ bin/elasticsearch -Ecluster.initial_master_nodes=master-a,master-b,master-c
--------------------------------------------------

It is technically sufficient to set this on a single master-eligible node in
the cluster, and only to mention that single node in the setting, but this
provides no fault tolerance before the cluster has fully formed. It
is therefore better to bootstrap using at least three master-eligible nodes.
In any case, when specifying the list of initial master nodes, **it is vitally
important** to configure each node with exactly the same list of nodes, to
prevent two independent clusters from forming. Typically you will set this on
the nodes that are mentioned in the list of initial master nodes.

NOTE: In alpha releases, all listed master-eligible nodes are required to be
  discovered before bootstrapping can take place. This requirement will be
  relaxed in production-ready releases.

WARNING: You must put exactly the same set of initial master nodes in each
  configuration file (or leave the configuration empty) in order to be sure
  that only a single cluster forms during bootstrapping and therefore to
  avoid the risk of data loss.

[float]
==== Choosing a cluster name

The `cluster.name` allows you to create multiple clusters which are separated
from each other. Nodes verify that they agree on their cluster name when they
first connect to each other, and if two nodes have different cluster names then
they will not communicate meaningfully and will not belong to the same cluster.
The default value for the cluster name is `elasticsearch`, but it is
recommended to change this to reflect the logical name of the cluster.

[float]
==== Auto-bootstrapping in development mode

If the cluster is running with a completely default configuration then it will
automatically bootstrap based on the nodes that could be discovered within a
short time after startup. Since nodes may not always reliably discover each
other quickly enough this automatic bootstrapping is not always reliable and
cannot be used in production deployments.

If any of the following settings are configured then auto-bootstrapping will
not take place, and you must configure `cluster.initial_master_nodes` as
described in the <<modules-discovery-bootstrap-cluster,section on cluster
bootstrapping>>:

* `discovery.zen.hosts_provider`
* `discovery.zen.ping.unicast.hosts`
* `cluster.initial_master_nodes`

[float]
[[modules-discovery-adding-removing-nodes]]
=== Adding and removing nodes

As nodes are added or removed Elasticsearch maintains an optimal level of fault
tolerance by updating the cluster's _voting configuration_, which is the set of
master-eligible nodes whose responses are counted when making decisions such as
electing a new master or committing a new cluster state.

It is recommended to have a small and fixed number of master-eligible nodes in a
cluster, and to scale the cluster up and down by adding and removing
master-ineligible nodes only. However there are situations in which it may be
desirable to add or remove some master-eligible nodes to or from a cluster.

If you wish to add some master-eligible nodes to your cluster, simply configure
the new nodes to find the existing cluster and start them up. Elasticsearch will
add the new nodes to the voting configuration if it is appropriate to do so.

When removing master-eligible nodes, it is important not to remove too many all
at the same time. For instance, if there are currently seven master-eligible
nodes and you wish to reduce this to three, it is not possible simply to stop
four of the nodes at once: to do so would leave only three nodes remaining,
which is less than half of the voting configuration, which means the cluster
cannot take any further actions.

As long as there are at least three master-eligible nodes in the cluster, as a
general rule it is best to remove nodes one-at-a-time, allowing enough time for
the cluster to <<modules-discovery-quorums,auto-adjust>> the voting
configuration and adapt the fault tolerance level to the new set of nodes.

If there are only two master-eligible nodes remaining then neither node can be
safely removed since both are required to reliably make progress, so you must
first inform Elasticsearch that one of the nodes should not be part of the voting
configuration, and that the voting power should instead be given to other nodes,
allowing the excluded node to be taken offline without preventing the other node
from making progress. A node which is added to a voting configuration exclusion
list still works normally, but Elasticsearch will try and remove it from the
voting configuration so its vote is no longer required, and will never
automatically move such a node back into the voting configuration after it has
been removed. Once a node has been successfully reconfigured out of the voting
configuration, it is safe to shut it down without affecting the cluster's
master-level availability. A node can be added to the voting configuration
exclusion list using the following API:

[source,js]
--------------------------------------------------
# Add node to voting configuration exclusions list and wait for the system to
# auto-reconfigure the node out of the voting configuration up to the default
# timeout of 30 seconds
POST /_cluster/voting_config_exclusions/node_name

# Add node to voting configuration exclusions list and wait for
# auto-reconfiguration up to one minute
POST /_cluster/voting_config_exclusions/node_name?timeout=1m
--------------------------------------------------
// CONSOLE

The node that should be added to the exclusions list is specified using
<<cluster-nodes,node filters>> in place of `node_name` here. If a call to the
voting configuration exclusions API fails then the call can safely be retried.
Only a successful response guarantees that the node has actually been removed
from the voting configuration and will not be reinstated.

Although the voting configuration exclusions API is most useful for down-scaling
a two-node to a one-node cluster, it is also possible to use it to remove
multiple master-eligible nodes all at the same time. Adding multiple nodes
to the exclusions list has the system try to auto-reconfigure all of these nodes
out of the voting configuration, allowing them to be safely shut down while
keeping the cluster available. In the example described above, shrinking a
seven-master-node cluster down to only have three master nodes, you could add
four nodes to the exclusions list, wait for confirmation, and then shut them
down simultaneously.

NOTE: Voting exclusions are only required when removing at least half of the
master-eligible nodes from a cluster in a short time period. They are not
required when removing master-ineligible nodes, nor are they required when
removing fewer than half of the master-eligible nodes.

Adding an exclusion for a node creates an entry for that node in the voting
configuration exclusions list, which has the system automatically try to
reconfigure the voting configuration to remove that node and prevents it from
returning to the voting configuration once it has removed. The current list of
exclusions is stored in the cluster state and can be inspected as follows:

[source,js]
--------------------------------------------------
GET /_cluster/state?filter_path=metadata.cluster_coordination.voting_config_exclusions
--------------------------------------------------
// CONSOLE

This list is limited in size by the following setting:

`cluster.max_voting_config_exclusions`::

    Sets a limits on the number of voting configuration exclusions at any one
    time.  Defaults to `10`.

Since voting configuration exclusions are persistent and limited in number, they
must be cleaned up. Normally an exclusion is added when performing some
maintenance on the cluster, and the exclusions should be cleaned up when the
maintenance is complete. Clusters should have no voting configuration exclusions
in normal operation.

If a node is excluded from the voting configuration because it is to be shut
down permanently then its exclusion can be removed once it has shut down and
been removed from the cluster. Exclusions can also be cleared if they were
created in error or were only required temporarily:

[source,js]
--------------------------------------------------
# Wait for all the nodes with voting configuration exclusions to be removed from
# the cluster and then remove all the exclusions, allowing any node to return to
# the voting configuration in the future.
DELETE /_cluster/voting_config_exclusions

# Immediately remove all the voting configuration exclusions, allowing any node
# to return to the voting configuration in the future.
DELETE /_cluster/voting_config_exclusions?wait_for_removal=false
--------------------------------------------------
// CONSOLE

[float]
[[cluster-state-publishing]]
=== Cluster state publishing

The master node is the only node in a cluster that can make changes to the
cluster state. The master node processes one cluster state update at a time,
applies the required changes and publishes the updated cluster state to all the
other nodes in the cluster. Each node receives the publish message, acknowledges
it, but does *not* yet apply it. If the master does not receive acknowledgement
from enough master-eligible nodes within a certain time (controlled by the
`cluster.publish.timeout` setting which defaults to 30 seconds) the cluster
state change is rejected.

Once enough nodes have responded, the cluster state is committed and a commit
message is sent to all the nodes. The nodes then proceed to apply the new
cluster state to their internal state. The master node waits for all nodes to
respond, or until `cluster.publish.timeout` has elapsed, before starting to
process the next update in the queue. The `cluster.publish.timeout` is measured
from the moment the publishing started.

If a node fails to apply a cluster state update within the
`cluster.publish.timeout` timeout then its cluster state lags behind the most
recently-published state from the master. The master waits for a further
timeout, `cluster.follower_lag.timeout`, which defaults to 90 seconds, and if
the node has still not successfully applied the cluster state update then it is
removed from the cluster.

NOTE: Elasticsearch is a peer to peer based system, in which nodes communicate
with one another directly. The high-throughput APIs (index, delete, search) do
not normally interact with the master node. The responsibility of the master
node is to maintain the global cluster state, and act if nodes join or leave the
cluster by reassigning shards. Each time the cluster state is changed, the new
state is published to all nodes in the cluster as described above.

[float]
[[no-master-block]]
=== No master block

For the cluster to be fully operational, it must have an active master.  The
`discovery.zen.no_master_block` settings controls what operations should be
rejected when there is no active master.

The `discovery.zen.no_master_block` setting has two valid values:

[horizontal]
`all`:: All operations on the node--i.e. both read & writes--will be rejected.
This also applies for api cluster state read or write operations, like the get
index settings, put mapping and cluster state api.
`write`:: (default) Write operations will be rejected. Read operations will
succeed, based on the last known cluster configuration.  This may result in
partial reads of stale data as this node may be isolated from the rest of the
cluster.

The `discovery.zen.no_master_block` setting doesn't apply to nodes-based APIs
(for example cluster stats, node info, and node stats APIs). Requests to these
APIs will not be blocked and can run on any available node.

[float]
[[master-election]]
=== Master Election

Elasticsearch uses an election process to agree on an elected master node, both
at startup and if the existing elected master fails. Any master-eligible node
can start an election, and normally the first election that takes place will
succeed. Elections only usually fail when two nodes both happen to start their
elections at about the same time, so elections are scheduled randomly on each
node to avoid this happening. Nodes will retry elections until a master is
elected, backing off on failure, so that eventually an election will succeed
(with arbitrarily high probability). The following settings control the
scheduling of elections.

`cluster.election.initial_timeout`::

    Sets the upper bound on how long a node will wait initially, or after the
    elected master fails, before attempting its first election. This defaults
    to `100ms`.

`cluster.election.back_off_time`::

    Sets the amount to increase the upper bound on the wait before an election
    on each election failure. Note that this is _linear_ backoff. This defaults
    to `100ms`

`cluster.election.max_timeout`::

    Sets the maximum upper bound on how long a node will wait before attempting
    an first election, so that an network partition that lasts for a long time
    does not result in excessively sparse elections. This defaults to `10s`

`cluster.election.duration`::

    Sets how long each election is allowed to take before a node considers it to
    have failed and schedules a retry. This defaults to `500ms`.

[float]
==== Joining an elected master

During master election, or when joining an existing formed cluster, a node will
send a join request to the master in order to be officially added to the
cluster. This join process can be configured with the following settings.

`cluster.join.timeout`::

    Sets how long a node will wait after sending a request to join a cluster
    before it considers the request to have failed and retries. Defaults to
    `60s`.

[float]
[[fault-detection]]
=== Fault Detection

An elected master periodically checks each of the nodes in the cluster in order
to ensure that they are still connected and healthy, and in turn each node in
the cluster periodically checks the health of the elected master. These checks
are known respectively as _follower checks_ and _leader checks_.

Elasticsearch allows for these checks occasionally to fail or timeout without
taking any action, and will only consider a node to be truly faulty after a
number of consecutive checks have failed. The following settings control the
behaviour of fault detection.

`cluster.fault_detection.follower_check.interval`::

    Sets how long the elected master waits between follower checks to each
    other node in the cluster. Defaults to `1s`.

`cluster.fault_detection.follower_check.timeout`::

    Sets how long the elected master waits for a response to a follower check
    before considering it to have failed. Defaults to `30s`.

`cluster.fault_detection.follower_check.retry_count`::

    Sets how many consecutive follower check failures must occur to each node
    before the elected master considers that node to be faulty and removes it
    from the cluster. Defaults to `3`.

`cluster.fault_detection.leader_check.interval`::

    Sets how long each node waits between checks of the elected master.
    Defaults to `1s`.

`cluster.fault_detection.leader_check.timeout`::

    Sets how long each node waits for a response to a leader check from the
    elected master before considering it to have failed. Defaults to `30s`.

`cluster.fault_detection.leader_check.retry_count`::

    Sets how many consecutive leader check failures must occur before a node
    considers the elected master to be faulty and attempts to find or elect a
    new master. Defaults to `3`.

If the elected master detects that a node has disconnected then this is treated
as an immediate failure, bypassing the timeouts and retries listed above, and
the master attempts to remove the node from the cluster. Similarly, if a node
detects that the elected master has disconnected then this is treated as an
immediate failure, bypassing the timeouts and retries listed above, and the
follower restarts its discovery phase to try and find or elect a new master.

[float]
[[modules-discovery-quorums]]
=== Quorum-based decision making

Electing a master node and changing the cluster state are the two fundamental
tasks that master-eligible nodes must work together to perform. It is important
that these activities work robustly even if some nodes have failed, and
Elasticsearch achieves this robustness by only considering each action to have
succeeded on receipt of responses from a _quorum_, a subset of the
master-eligible nodes in the cluster. The advantage of requiring only a subset
of the nodes to respond is that it allows for some of the nodes to fail without
preventing the cluster from making progress, and the quorums are carefully
chosen so as not to allow the cluster to "split brain", i.e. to be partitioned
into two pieces each of which may make decisions that are inconsistent with
those of the other piece.

Elasticsearch allows you to add and remove master-eligible nodes to a running
cluster. In many cases you can do this simply by starting or stopping the nodes
as required, as described in more detail in the
<<modules-discovery-adding-removing-nodes,section on adding and removing
nodes>>.

As nodes are added or removed Elasticsearch maintains an optimal level of fault
tolerance by updating the cluster's _voting configuration_, which is the set of
master-eligible nodes whose responses are counted when making decisions such as
electing a new master or committing a new cluster state. A decision is only made
once more than half of the nodes in the voting configuration have responded.
Usually the voting configuration is the same as the set of all the
master-eligible nodes that are currently in the cluster, but there are some
situations in which they may be different.

To be sure that the cluster remains available you **must not stop half or more
of the nodes in the voting configuration at the same time**. As long as more
than half of the voting nodes are available the cluster can still work normally.
This means that if there are three or four master-eligible nodes then the
cluster can tolerate one of them being unavailable; if there are two or fewer
master-eligible nodes then they must all remain available.

After a node has joined or left the cluster the elected master must issue a
cluster-state update that adjusts the voting configuration to match, and this
can take a short time to complete. It is important to wait for this adjustment
to complete before removing more nodes from the cluster.

[float]
==== Setting the initial quorum

When a brand-new cluster starts up for the first time, one of the tasks it must
perform is to elect its first master node, for which it needs to know the set of
master-eligible nodes whose votes should count in this first election. This
initial voting configuration is known as the _bootstrap configuration_.

It is important that the bootstrap configuration identifies exactly which nodes
should vote in the first election, and it is not sufficient to configure each
node with an expectation of how many nodes there should be in the cluster. It is
also important to note that the bootstrap configuration must come from outside
the cluster: there is no safe way for the cluster to determine the bootstrap
configuration correctly on its own.

If the bootstrap configuration is not set correctly then there is a risk when
starting up a brand-new cluster is that you accidentally form two separate
clusters instead of one. This could lead to data loss: you might start using
both clusters before noticing that anything had gone wrong, and it will then be
impossible to merge them together later.

NOTE: To illustrate the problem with configuring each node to expect a certain
cluster size, imagine starting up a three-node cluster in which each node knows
that it is going to be part of a three-node cluster. A majority of three nodes
is two, so normally the first two nodes to discover each other will form a
cluster and the third node will join them a short time later. However, imagine
that four nodes were erroneously started instead of three: in this case there
are enough nodes to form two separate clusters. Of course if each node is
started manually then it's unlikely that too many nodes are started, but it's
certainly possible to get into this situation if using a more automated
orchestrator, particularly if the orchestrator is not resilient to failures such
as network partitions.

The <<modules-discovery-bootstrap-cluster,cluster bootstrapping process>> is
only required the very first time a whole cluster starts up: new nodes joining
an established cluster can safely obtain all the information they need from the
elected master, and nodes that have previously been part of a cluster will have
stored to disk all the information required when restarting.

[float]
==== Cluster maintenance, rolling restarts and migrations

Many cluster maintenance tasks involve temporarily shutting down one or more
nodes and then starting them back up again. By default Elasticsearch can remain
available if one of its master-eligible nodes is taken offline, such as during a
<<rolling-upgrades,rolling restart>>. Furthermore, if multiple nodes are stopped
and then started again then it will automatically recover, such as during a
<<restart-upgrade,full cluster restart>>. There is no need to take any further
action with the APIs described here in these cases, because the set of master
nodes is not changing permanently.

It is also possible to perform a migration of a cluster onto entirely new nodes
without taking the cluster offline, via a _rolling migration_. A rolling
migration is similar to a rolling restart, in that it is performed one node at a
time, and also requires no special handling for the master-eligible nodes as
long as there are at least two of them available at all times.

TODO the above is only true if the maintenance happens slowly enough, otherwise
the configuration might not catch up. Need to add this to the rolling restart
docs.

[float]
==== Auto-reconfiguration

Nodes may join or leave the cluster, and Elasticsearch reacts by making
corresponding changes to the voting configuration in order to ensure that the
cluster is as resilient as possible. The default auto-reconfiguration behaviour
is expected to give the best results in most situation. The current voting
configuration is stored in the cluster state so you can inspect its current
contents as follows:

[source,js]
--------------------------------------------------
GET /_cluster/state?filter_path=metadata.cluster_coordination.last_committed_config
--------------------------------------------------
// CONSOLE

NOTE: The current voting configuration is not necessarily the same as the set of
all available master-eligible nodes in the cluster. Altering the voting
configuration itself involves taking a vote, so it takes some time to adjust the
configuration as nodes join or leave the cluster. Also, there are situations
where the most resilient configuration includes unavailable nodes, or does not
include some available nodes, and in these situations the voting configuration
will differ from the set of available master-eligible nodes in the cluster.

Larger voting configurations are usually more resilient, so Elasticsearch will
normally prefer to add master-eligible nodes to the voting configuration once
they have joined the cluster. Similarly, if a node in the voting configuration
leaves the cluster and there is another master-eligible node in the cluster that
is not in the voting configuration then it is preferable to swap these two nodes
over, leaving the size of the voting configuration unchanged but increasing its
resilience.

It is not so straightforward to automatically remove nodes from the voting
configuration after they have left the cluster, and different strategies have
different benefits and drawbacks, so the right choice depends on how the cluster
will be used and is controlled by the following setting.

`cluster.auto_shrink_voting_configuration`::

    Defaults to `true`, meaning that the voting configuration will automatically
    shrink, shedding departed nodes, as long as it still contains at least 3
    nodes.  If set to `false`, the voting configuration never automatically
    shrinks; departed nodes must be removed manually using the
    <<modules-discovery-adding-removing-nodes,voting configuration exclusions API>>.

NOTE: If `cluster.auto_shrink_voting_configuration` is set to `true`, the
recommended and default setting, and there are at least three master-eligible
nodes in the cluster, then Elasticsearch remains capable of processing
cluster-state updates as long as all but one of its master-eligible nodes are
healthy.

There are situations in which Elasticsearch might tolerate the loss of multiple
nodes, but this is not guaranteed under all sequences of failures. If this
setting is set to `false` then departed nodes must be removed from the voting
configuration manually, using the vote withdrawal API described below, to
achieve the desired level of resilience.

Note that Elasticsearch will not suffer from a "split-brain" inconsistency
however it is configured. This setting only affects its availability in the
event of the failure of some of its nodes, and the administrative tasks that
must be performed as nodes join and leave the cluster.

[float]
==== Even numbers of master-eligible nodes

There should normally be an odd number of master-eligible nodes in a cluster.
If there is an even number then Elasticsearch will leave one of them out of the
voting configuration to ensure that it has an odd size. This does not decrease
the failure-tolerance of the cluster, and in fact improves it slightly: if the
cluster is partitioned into two even halves then one of the halves will contain
a majority of the voting configuration and will be able to keep operating,
whereas if all of the master-eligible nodes' votes were counted then neither
side could make any progress in this situation.

For instance if there are four master-eligible nodes in the cluster and the
voting configuration contained all of them then any quorum-based decision would
require votes from at least three of them, which means that the cluster can only
tolerate the loss of a single master-eligible node. If this cluster were split
into two equal halves then neither half would contain three master-eligible
nodes so would not be able to make any progress. However if the voting
configuration contains only three of the four master-eligible nodes then the
cluster is still only fully tolerant to the loss of one node, but quorum-based
decisions require votes from two of the three voting nodes. In the event of an
even split, one half will contain two of the three voting nodes so will remain
available.

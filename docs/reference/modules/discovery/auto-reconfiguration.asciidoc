[float]
==== Adding and removing nodes

As nodes are added or removed Elasticsearch maintains an optimal level of fault
tolerance by updating the cluster's _voting configuration_, which is the set of
master-eligible nodes whose responses are counted when making decisions such as
electing a new master or committing a new cluster state.

It is recommended to have a small and fixed number of master-eligible nodes in
a cluster, and to scale the cluster up and down by adding and removing
non-master-eligible nodes only. However there are situations in which it may be
desirable to add or remove some master-eligible nodes to or from a cluster.

If you wish to add some master-eligible nodes to your cluster, simply configure
the new nodes to find the existing cluster and start them up. Elasticsearch
will add the new nodes to the voting configuration if it is appropriate to do
so.

When removing master-eligible nodes, it is important not to remove too many all
at the same time. For instance, if there are currently seven master-eligible
nodes and you wish to reduce this to three, it is not possible simply to stop
four of the nodes at once: to do so would leave only three nodes remaining,
which is less than half of the voting configuration, which means the cluster
cannot take any further actions.

As long as there are at least three master-eligible nodes in the cluster, as a
general rule it is best to remove nodes one-at-a-time, allowing enough time for
the auto-reconfiguration to take effect after each removal.

If there are only two master-eligible nodes then neither node can be safely
removed since both are required to reliably make progress, so you must first
inform Elasticsearch that one of the nodes should not be part of the voting
configuration, and that the voting power should instead be given to other
nodes, allowing the excluded node to be taken offline without preventing the
other node from making progress. A node which is added to a voting
configuration exclusion list still works normally, but Elasticsearch will try
and remove it from the voting configuration so its vote is no longer required,
and will never automatically move such a node back into the voting
configuration after it has been removed. Once a node has been successfully
reconfigured out of the voting configuration, it is safe to shut it down
without affecting the cluster's availability. A node can be added to the voting
configuration exclusion list using the following API:

[source,js]
--------------------------------------------------
# Add node to voting configuration exclusions list and wait for the system to
# auto-reconfigure the node out of the voting configuration up to the default
# timeout of 30 seconds
POST /_cluster/voting_config_exclusions/node_name
# Add node to voting configuration exclusions list and wait for
# auto-reconfiguration up to one minute
POST /_cluster/voting_config_exclusions/node_name?timeout=1m
--------------------------------------------------
// CONSOLE

The node that should be added to the exclusions list is specified using
<<cluster-nodes,node filters>> in place of `node_name` here. If a call to the
voting configuration exclusions API fails then the call can safely be retried.
A successful response guarantees that the node has been removed from the voting
configuration and will not be reinstated.

Although the voting configuration exclusions API is most useful for
down-scaling a two-node to a one-node cluster, it is also possible to use it to
remove multiple nodes from larger clusters all at the same time. Adding
multiple nodes to the exclusions list has the system try to auto-reconfigure
all of these nodes out of the voting configuration, allowing them to be safely
shut down while keeping the cluster available. In the example described above,
shrinking a seven-master-node cluster down to only have three master nodes, you
could add four nodes to the exclusions list, wait for confirmation, and then
shut them down simultaneously.

Adding an exclusion for a node creates an entry for that node in the voting
configuration exclusions list, which has the system automatically try to
reconfigure the voting configuration to remove that node and prevents it from
returning to the voting configuration once it has removed. The current set of
exclusions is stored in the cluster state and can be inspected as follows:

[source,js]
--------------------------------------------------
GET /_cluster/state?filter_path=metadata.cluster_coordination.voting_config_exclusions
--------------------------------------------------
// CONSOLE

This list is limited in size by the following setting:

`cluster.max_voting_config_exclusions`::

    Sets a limits on the number of voting configuration exclusions at any one
    time.  Defaults to `10`.

Since voting configuration exclusions are persistent and limited in number,
they must be cleaned up. Normally an exclusion is added when performing some
maintenance on the cluster, and the exclusions should be cleaned up when the
maintenance is complete. Clusters should have no voting configuration
exclusions in normal operation.

If a node is excluded from the voting configuration because it is to be shut
down permanently then its exclusion can be removed once it has shut down and
been removed from the cluster. Exclusions can also be cleared if they were
created in error or were only required temporarily:

[source,js]
--------------------------------------------------
# Wait for all the nodes with voting configuration exclusions to be removed
# from the cluster and then remove all the exclusions, allowing any node to
# return to the voting configuration in the future.
DELETE /_cluster/voting_config_exclusions
# Immediately remove all the voting configuration exclusions, allowing any node
# to return to the voting configuration in the future.
DELETE /_cluster/voting_config_exclusions?wait_for_removal=false
--------------------------------------------------
// CONSOLE

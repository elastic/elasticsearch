[[modules-discovery-quorums]]
=== Quorum-based decision making

Electing a master node and changing the cluster state are the two fundamental
tasks that master-eligible nodes must work together to perform. It is important
that these activities work robustly even if some nodes have failed, and
Elasticsearch achieves this robustness by only considering each action to have
succeeded on receipt of responses from a _quorum_, a subset of the
master-eligible nodes in the cluster. The advantage of requiring only a subset
of the nodes to respond is that it allows for some of the nodes to fail without
preventing the cluster from making progress, and the quorums are carefully
chosen so as not to allow the cluster to "split brain", i.e. to be partitioned
into two pieces each of which may make decisions that are inconsistent with
those of the other piece.

Elasticsearch allows you to add and remove master-eligible nodes to a running
cluster. In many cases you can do this simply by starting or stopping the nodes
as required, as described in more detail below.

As nodes are added or removed Elasticsearch maintains an optimal level of fault
tolerance by updating the cluster's _voting configuration_, which is the set of
master-eligible nodes whose responses are counted when making decisions such as
electing a new master or committing a new cluster state. A decision is only
made once more than half of the nodes in the voting configuration have
responded. Usually the voting configuration is the same as the set of all the
master-eligible nodes that are currently in the cluster, but there are some
situations in which they may be different.

To be sure that the cluster remains available you **must not stop half or more
of the nodes in the voting configuration at the same time**. As long as more
than half of the voting nodes are available the cluster can still work
normally. This means that if there are three or four master-eligible nodes then
the cluster can tolerate one of them being unavailable; if there are two or
fewer master-eligible nodes then they must all remain available.

After a node has joined or left the cluster the elected master must issue a
cluster-state update that adjusts the voting configuration to match, and this
can take a short time to complete. It is important to wait for this adjustment
to complete before removing more nodes from the cluster.

[float]
=== Getting the initial quorum

When a brand-new cluster starts up for the first time, one of the tasks it must
perform is to elect its first master node, for which it needs to know the set
of master-eligible nodes whose votes should count in this first election. This
initial voting configuration is known as the _bootstrap configuration_.

It is important that the bootstrap configuration identifies exactly which nodes
should vote in the first election, and it is not sufficient to configure each
node with an expectation of how many nodes there should be in the cluster. It
is also important to note that the bootstrap configuration must come from
outside the cluster: there is no safe way for the cluster to determine the
bootstrap configuration correctly on its own.

If the bootstrap configuration is not set correctly then there is a risk when
starting up a brand-new cluster is that you accidentally form two separate
clusters instead of one. This could lead to data loss: you might start using
both clusters before noticing that anything had gone wrong, and it will then be
impossible to merge them together later.

NOTE: To illustrate the problem with configuring each node to expect a certain
cluster size, imagine starting up a three-node cluster in which each node knows
that it is going to be part of a three-node cluster. A majority of three nodes
is two, so normally the first two nodes to discover each other will form a
cluster and the third node will join them a short time later. However, imagine
that four nodes were erroneously started instead of three: in this case there
are enough nodes to form two separate clusters. Of course if each node is
started manually then it's unlikely that too many nodes are started, but it's
certainly possible to get into this situation if using a more automated
orchestrator, particularly if the orchestrator is not resilient to failures
such as network partitions.

The cluster bootstrapping process is only required the very first time a whole
cluster starts up: new nodes joining an established cluster can safely obtain
all the information they need from the elected master, and nodes that have
previously been part of a cluster will have stored to disk all the information
required when restarting.

[float]
=== Cluster maintenance, rolling restarts and migrations

Many cluster maintenance tasks involve temporarily shutting down one or more
nodes and then starting them back up again. By default Elasticsearch can remain
available if one of its master-eligible nodes is taken offline, such as during
a <<rolling-upgrades,rolling restart>>. Furthermore, if multiple nodes are
stopped and then started again then it will automatically recover, such as
during a <<restart-upgrade,full cluster restart>>. There is no need to take any
further action with the APIs described here in these cases, because the set of
master nodes is not changing permanently.

It is also possible to perform a migration of a cluster onto entirely new nodes
without taking the cluster offline, via a _rolling migration_. A rolling
migration is similar to a rolling restart, in that it is performed one node at
a time, and also requires no special handling for the master-eligible nodes as
long as there are at least two of them available at all times.

TODO the above is only true if the maintenance happens slowly enough, otherwise
the configuration might not catch up. Need to add this to the rolling restart
docs.

[float]
==== Auto-reconfiguration

Nodes may join or leave the cluster, and Elasticsearch reacts by making
corresponding changes to the voting configuration in order to ensure that the
cluster is as resilient as possible. The default auto-reconfiguration behaviour
is expected to give the best results in most situation. The current voting
configuration is stored in the cluster state so you can inspect its current
contents as follows:

[source,js]
--------------------------------------------------
GET /_cluster/state?filter_path=metadata.cluster_coordination.last_committed_config
--------------------------------------------------
// CONSOLE

NOTE: The current voting configuration is not necessarily the same as the set
of all available master-eligible nodes in the cluster. Altering the voting
configuration itself involves taking a vote, so it takes some time to adjust
the configuration as nodes join or leave the cluster. Also, there are
situations where the most resilient configuration includes unavailable nodes,
or does not include some available nodes, and in these situations the voting
configuration will differ from the set of available master-eligible nodes in
the cluster.

Larger voting configurations are usually more resilient, so Elasticsearch will
normally prefer to add master-eligible nodes to the voting configuration once
they have joined the cluster. Similarly, if a node in the voting configuration
leaves the cluster and there is another master-eligible node in the cluster
that is not in the voting configuration then it is preferable to swap these two
nodes over, leaving the size of the voting configuration unchanged but
increasing its resilience.

It is not so straightforward to automatically remove nodes from the voting
configuration after they have left the cluster, and different strategies have
different benefits and drawbacks, so the right choice depends on how the
cluster will be used and is controlled by the following setting.

`cluster.auto_shrink_voting_configuration`::

    Defaults to `true`, meaning that the voting configuration will
    automatically shrink, shedding departed nodes, as long as it still contains
    at least 3 nodes.  If set to `false`, the voting configuration never
    automatically shrinks; departed nodes must be removed manually using the
    vote withdrawal API described below.

NOTE: If `cluster.auto_shrink_voting_configuration` is set to `true`, the
recommended and default setting, and there are at least three master-eligible
nodes in the cluster, then Elasticsearch remains capable of processing
cluster-state updates as long as all but one of its master-eligible nodes are
healthy.

There are situations in which Elasticsearch might tolerate the loss of multiple
nodes, but this is not guaranteed under all sequences of failures. If this
setting is set to `false` then departed nodes must be removed from the voting
configuration manually, using the vote withdrawal API described below, to achieve
the desired level of resilience.

Note that Elasticsearch will not suffer from a "split-brain" inconsistency
however it is configured. This setting only affects its availability in the
event of the failure of some of its nodes, and the administrative tasks that
must be performed as nodes join and leave the cluster.

[float]
==== Even numbers of master-eligible nodes

There should normally be an odd number of master-eligible nodes in a cluster.
If there is an even number then Elasticsearch will leave one of them out of the
voting configuration to ensure that it has an odd size. This does not decrease
the failure-tolerance of the cluster, and in fact improves it slightly: if the
cluster is partitioned into two even halves then one of the halves will contain
a majority of the voting configuration and will be able to keep operating,
whereas if all of the master-eligible nodes' votes were counted then neither
side could make any progress in this situation.

For instance if there are four master-eligible nodes in the cluster and the
voting configuration contained all of them then any quorum-based decision would
require votes from at least three of them, which means that the cluster can
only tolerate the loss of a single master-eligible node. If this cluster were
split into two equal halves then neither half would contain three
master-eligible nodes so would not be able to make any progress. However if the
voting configuration contains only three of the four master-eligible nodes then
the cluster is still only fully tolerant to the loss of one node, but
quorum-based decisions require votes from two of the three voting nodes. In the
event of an even split, one half will contain two of the three voting nodes so
will remain available.

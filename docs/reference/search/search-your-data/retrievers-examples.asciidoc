[[retrievers-examples]]
=== Retrievers examples

Learn how to combine different retrievers in these hands-on examples.
To demonstrate the full functionality of retrievers, these examples require access to a <<semantic-reranking-models,semantic reranking model>> set up using the <<inference-apis,Elastic inference APIs>>.

[discrete]
[[retrievers-examples-setup]]
==== Add example data

To begin with, we'll set up the necessary services and have them in place for later use.

[source,console]
----
PUT _inference/rerank/my-rerank-model
{
 "service": "cohere",
 "service_settings": {
   "model_id": "rerank-english-v3.0",
   "api_key": "{{COHERE_API_KEY}}"
 }
}
----
//NOTCONSOLE

Now that we have our reranking service in place, lets create the `retrievers_example` index, and add some documents to it.
[source,console]
----
PUT retrievers_example
{
   "mappings": {
       "properties": {
           "vector": {
               "type": "dense_vector",
               "dims": 3,
               "similarity": "l2_norm",
               "index": true
           },
           "text": {
               "type": "text"
           },
           "year": {
               "type": "integer"
           },
           "topic": {
               "type": "keyword"
           }
       }
   }
}

POST /retrievers_example/_doc/1
{
 "vector": [0.23, 0.67, 0.89],
 "text": "Large language models are revolutionizing information retrieval by boosting search precision, deepening contextual understanding, and reshaping user experiences in data-rich environments.",
 "year": 2024,
 "topic": ["llm", "ai", "information_retrieval"]
}

POST /retrievers_example/_doc/2
{
 "vector": [0.12, 0.56, 0.78],
 "text": "Artificial intelligence is transforming medicine, from advancing diagnostics and tailoring treatment plans to empowering predictive patient care for improved health outcomes.",
 "year": 2023,
 "topic": ["ai", "medicine"]
}

POST /retrievers_example/_doc/3
{
 "vector": [0.45, 0.32, 0.91],
  "text": "AI is redefining security by enabling advanced threat detection, proactive risk analysis, and dynamic defenses against increasingly sophisticated cyber threats.",
 "year": 2024,
 "topic": ["ai", "security"]
}

POST /retrievers_example/_doc/4
{
 "vector": [0.34, 0.21, 0.98],
 "text": "Elastic introduces Elastic AI Assistant, the open, generative AI sidekick powered by ESRE to democratize cybersecurity and enable users of every skill level.",
 "year": 2023,
 "topic": ["ai", "elastic", "assistant"]
}

POST /retrievers_example/_doc/5
{
 "vector": [0.11, 0.65, 0.47],
 "text": "Learn how to spin up a deployment of our hosted Elasticsearch Service and use Elastic Observability to gain deeper insight into the behavior of your applications and systems.",
 "year": 2024,
 "topic": ["documentation", "observability", "elastic"]
}

POST /retrievers_example/_refresh

----
// NOTCONSOLE

Now that we also have our documents in place, let's try to run some queries using retrievers.

[discrete]
[[retrievers-examples-combining-standard-knn-retrievers-with-rrf]]
==== Example: Combining query and kNN with RRF

First, let's examine how to combine two different types of queries: a `kNN` query and a
`query_string` query. While these queries may produce scores in different ranges, we can use
Reciprocal Rank Fusion (`rrf`) to combine the results and generate a merged final result
list.

To implement this in the retriever framework, we start with the top-level element: our `rrf`
retriever. This retriever operates on top of two other retrievers: a `knn` retriever and a
`standard` retriever. Our query structure would look like this:

[source,console]
----
GET /retrievers_example/_search
{
   "retriever":{
       "rrf": {
           "retrievers":[
               {
                   "standard":{
                       "query":{
                           "query_string":{
                              "query": "(information retrieval) OR (artificial intelligence)",
                              "default_field": "text"
                           }
                       }
                   }
               },
               {
                   "knn": {
                       "field": "vector",
                       "query_vector": [
                           0.23,
                           0.67,
                           0.89
                       ],
                       "k": 3,
                       "num_candidates": 5
                   }
               }
           ],
           "rank_window_size": 10,
           "rank_constant": 1
       }
   },
   "_source": false
}
----
// TEST[setup:retrievers_examples]

Which would return the following response based on the final rrf score for each result

[source,console-result]
----
{
    "took": 42,
    "timed_out": false,
    "_shards": {
        "total": 1,
        "successful": 1,
        "skipped": 0,
        "failed": 0
    },
    "hits": {
        "total": {
            "value": 3,
            "relation": "eq"
        },
        "max_score": 0.8333334,
        "hits": [
            {
                "_index": "retrievers_example",
                "_id": "1",
                "_score": 0.8333334
            },
            {
                "_index": "retrievers_example",
                "_id": "2",
                "_score": 0.8333334
            },
            {
                "_index": "retrievers_example",
                "_id": "3",
                "_score": 0.25
            }
        ]
    }
}
----
// TESTRESPONSE[s/"took": 42/"took": $body.took/]

[discrete]
[[retrievers-examples-collapsing-retriever-results]]
==== Example: Grouping results by year with `collapse`

In our result set, we have many documents with the same `year` value. We can clean this
up using the `collapse` parameter with our retriever. This enables grouping results by
any field and returns only the highest-scoring document from each group. In this example
we'll collapse our results based on the `year` field.

[source,console]
----
GET /retrievers_example/_search
{
   "retriever":{
       "rrf": {
           "retrievers":[
               {
                   "standard":{
                       "query":{
                           "query_string":{
                              "query": "(information retrieval) OR (artificial intelligence)",
                              "default_field": "text"
                           }
                       }
                   }
               },
               {
                   "knn": {
                       "field": "vector",
                       "query_vector": [
                           0.23,
                           0.67,
                           0.89
                       ],
                       "k": 3,
                       "num_candidates": 5
                   }
               }
           ],
           "rank_window_size": 10,
           "rank_constant": 1
       }
   },
   "collapse": {
       "field": "year",
       "inner_hits": {
           "name": "topic related documents",
           "_source": ["year"]
       }
   },
    "_source": false
}
----
// TEST[setup:retrievers_examples]

Which would return the following response collapsed results

[source,console-result]
----
{
    "took": 42,
    "timed_out": false,
    "_shards": {
        "total": 1,
        "successful": 1,
        "skipped": 0,
        "failed": 0
    },
    "hits": {
        "total": {
            "value": 3,
            "relation": "eq"
        },
        "max_score": 0.8333334,
        "hits": [
            {
                "_index": "retrievers_example",
                "_id": "1",
                "_score": 0.8333334,
                "fields": {
                    "year": [
                        2024
                    ]
                },
                "inner_hits": {
                    "topic related documents": {
                        "hits": {
                            "total": {
                                "value": 2,
                                "relation": "eq"
                            },
                            "max_score": 0.8333334,
                            "hits": [
                                {
                                    "_index": "retrievers_example",
                                    "_id": "1",
                                    "_score": 0.8333334,
                                    "_source": {
                                        "year": 2024
                                    }
                                },
                                {
                                    "_index": "retrievers_example",
                                    "_id": "3",
                                    "_score": 0.25,
                                    "_source": {
                                        "year": 2024
                                    }
                                }
                            ]
                        }
                    }
                }
            },
            {
                "_index": "retrievers_example",
                "_id": "2",
                "_score": 0.8333334,
                "fields": {
                    "year": [
                        2023
                    ]
                },
                "inner_hits": {
                    "topic related documents": {
                        "hits": {
                            "total": {
                                "value": 1,
                                "relation": "eq"
                            },
                            "max_score": 0.8333334,
                            "hits": [
                                {
                                    "_index": "retrievers_example",
                                    "_id": "2",
                                    "_score": 0.8333334,
                                    "_source": {
                                        "year": 2023
                                    }
                                }
                            ]
                        }
                    }
                }
            }
        ]
    }
}
----
// TESTRESPONSE[s/"took": 42/"took": $body.took/]

[discrete]
[[retrievers-examples-text-similarity-reranker-on-top-of-rrf]]
==== Example: Rerank results of an RRF retriever

Previously, we used a `text_similarity_reranker` retriever within an `rrf` retriever.
Because retrievers support full composability, we can also rerank the results of an
`rrf` retriever. Let's apply this to our first example.

[source,console]
----
GET retrievers_example/_search
{
   "retriever": {
       "text_similarity_reranker": {
           "retriever": {
               "rrf": {
                   "retrievers": [
                       {
                           "standard":{
                               "query":{
                                   "query_string":{
                                      "query": "(information retrieval) OR (artificial intelligence)",
                                      "default_field": "text"
                                   }
                               }
                           }
                       },
                       {
                           "knn": {
                               "field": "vector",
                               "query_vector": [
                                   0.23,
                                   0.67,
                                   0.89
                               ],
                               "k": 3,
                               "num_candidates": 5
                           }
                       }
                   ],
                   "rank_window_size": 10,
                   "rank_constant": 1
               }
           },
           "field": "text",
           "inference_id": "my-rerank-model",
           "inference_text": "What are the state of the art applications of AI in information retrieval?"
       }
   },
   "_source": false
}

----
// TEST[skip:no_access_to_reranker]

[discrete]
[[retrievers-examples-rrf-ranking-on-text-similarity-reranker-results]]
==== Example: RRF with semantic reranker

For this example, we'll replace our semantic query with the `my-rerank-model`
reranker we previously configured. Since this is a reranker, it needs an initial pool of
documents to work with. In this case, we'll filter for documents about `ai` topics.

[source,console]
----
GET /retrievers_example/_search
{
    "retriever": {
        "rrf": {
            "retrievers": [
                {
                    "knn": {
                        "field": "vector",
                        "query_vector": [
                            0.23,
                            0.67,
                            0.89
                        ],
                        "k": 3,
                        "num_candidates": 5
                    }
                },
                {
                    "text_similarity_reranker": {
                        "retriever": {
                            "standard": {
                                "query": {
                                    "term": {
                                        "topic": "ai"
                                    }
                                }
                            }
                        },
                        "field": "text",
                        "inference_id": "my-rerank-model",
                        "inference_text": "Can I use generative AI to identify user intent and improve search relevance?"
                    }
                }
            ],
            "rank_window_size": 10,
            "rank_constant": 1
        }
    },
    "_source": false
}
----
// TEST[skip:no_access_to_reranker]

[discrete]
[[retrievers-examples-chaining-text-similarity-reranker-retrievers]]
==== Example: Chaining multiple semantic rerankers

Full composability means we can chain together multiple retrievers of the same type. For instance, imagine we have a computationally expensive reranker that's specialized for AI content. We can rerank the results of a `text_similarity_reranker` using another `text_similarity_reranker` retriever. Each reranker can operate on different fields and/or use different inference services.

[source,console]
----
GET retrievers_example/_search
{
   "retriever": {
       "text_similarity_reranker": {
           "retriever": {
               "text_similarity_reranker": {
                   "retriever": {
                       "knn": {
                           "field": "vector",
                           "query_vector": [
                               0.23,
                               0.67,
                               0.89
                           ],
                           "k": 3,
                           "num_candidates": 5
                       }
                   },
                   "rank_window_size": 100,
                   "field": "text",
                   "inference_id": "my-rerank-model",
                   "inference_text": "What are the state of the art applications of AI in information retrieval?"
               }
           },
           "rank_window_size": 10,
           "field": "text",
           "inference_id": "my-other-more-expensive-rerank-model",
           "inference_text": "Applications of Large Language Models in technology and their impact on user satisfaction"
       }
   },
    "_source": false
}
----
// TEST[skip:no_access_to_reranker]

Note that our example applies two reranking steps. First, we rerank the top 100
documents from the `knn` search using the `my-rerank-model` reranker. Then we
pick the top 10 results and rerank them using the more fine-grained
`my-other-more-expensive-rerank-model`.

[discrete]
[[retrievers-examples-rrf-and-aggregations]]
==== Example: Combine RRF with aggregations

Retrievers support both composability and most of the standard `_search` functionality. For instance,
we can compute aggregations with the `rrf` retriever. When using a compound retriever,
the aggregations are computed based on its nested retrievers. In the following example,
the `terms` aggregation for the `topic` field will include all results, not just the top `rank_window_size`,
from the 2 nested retrievers, i.e. all documents whose `year` field is greater than 2023, and whose `topic` field
matches the term `elastic`.

[source,console]
----
GET retrievers_example/_search
{
    "retriever": {
        "rrf": {
            "retrievers": [
                {
                    "standard": {
                        "query": {
                            "range": {
                                "year": {
                                    "gt": 2023
                                }
                            }
                        }
                    }
                },
                {
                    "standard": {
                        "query": {
                            "term": {
                                "topic": "elastic"
                            }
                        }
                    }
                }
            ],
            "rank_window_size": 10,
            "rank_constant": 1
        }
    },
    "_source": false,
    "aggs": {
        "topics": {
            "terms": {
                "field": "topic"
            }
        }
    }
}
----
// TEST[setup:retrievers_examples]

The output of which would look like the following:
[source, console-result]
----
{
    "took": 42,
    "timed_out": false,
    "_shards": {
        "total": 1,
        "successful": 1,
        "skipped": 0,
        "failed": 0
    },
    "hits": {
        "total": {
            "value": 4,
            "relation": "eq"
        },
        "max_score": 0.5833334,
        "hits": [
            {
                "_index": "retrievers_example",
                "_id": "5",
                "_score": 0.5833334
            },
            {
                "_index": "retrievers_example",
                "_id": "1",
                "_score": 0.5
            },
            {
                "_index": "retrievers_example",
                "_id": "4",
                "_score": 0.5
            },
            {
                "_index": "retrievers_example",
                "_id": "3",
                "_score": 0.33333334
            }
        ]
    },
    "aggregations": {
        "topics": {
            "doc_count_error_upper_bound": 0,
            "sum_other_doc_count": 0,
            "buckets": [
                {
                    "key": "ai",
                    "doc_count": 3
                },
                {
                    "key": "elastic",
                    "doc_count": 2
                },
                {
                    "key": "assistant",
                    "doc_count": 1
                },
                {
                    "key": "documentation",
                    "doc_count": 1
                },
                {
                    "key": "information_retrieval",
                    "doc_count": 1
                },
                {
                    "key": "llm",
                    "doc_count": 1
                },
                {
                    "key": "observability",
                    "doc_count": 1
                },
                {
                    "key": "security",
                    "doc_count": 1
                }
            ]
        }
    }
}
----
// TESTRESPONSE[s/"took": 42/"took": $body.took/]


[discrete]
[[retrievers-examples-explain-multiple-rrf]]
==== Example: Explainability with multiple retrievers
By adding `explain: true` to the request, each retriever will now provide a detailed explanation of all the steps
and calculations that took place for the final score to be computed. Composability is fully supported as well in the context of `explain`, and
each retriever will provide its own explanation, as we can see in the example below

[source,console]
----
GET /retrievers_example/_search
{
    "retriever": {
        "rrf": {
            "retrievers": [
                {
                    "standard": {
                        "query": {
                            "term": {
                                "topic": "elastic"
                            }
                        }
                    }
                },
                {
                    "rrf": {
                        "retrievers": [
                            {
                                "standard": {
                                    "query": {
                                        "query_string": {
                                            "query": "(information retrieval) OR (artificial intelligence)",
                                            "default_field": "text"
                                        }
                                    }
                                }
                            },
                            {
                                "knn": {
                                    "field": "vector",
                                    "query_vector": [
                                        0.23,
                                        0.67,
                                        0.89
                                    ],
                                    "k": 3,
                                    "num_candidates": 5
                                }
                            }
                        ],
                        "rank_window_size": 10,
                        "rank_constant": 1
                    }
                }
            ],
            "rank_window_size": 10,
            "rank_constant": 1
        }
    },
    "_source": false,
    "size": 1,
    "explain": true
}
----
// TEST[setup:retrievers_examples]

The output of which, albeit a bit verbose, will provide all the necessary info to assist in debugging and reason with ranking
[source, console-result]
----
{
    "took": 42,
    "timed_out": false,
    "_shards": {
        "total": 1,
        "successful": 1,
        "skipped": 0,
        "failed": 0
    },
    "hits": {
        "total": {
            "value": 5,
            "relation": "eq"
        },
        "max_score": 0.5,
        "hits": [
            {
                "_shard": "[retrievers_example][0]",
                "_node": "jn_rdZFKS3-UgWVsVdj2Vg",
                "_index": "retrievers_example",
                "_id": "1",
                "_score": 0.5,
                "_explanation": {
                    "value": 0.5,
                    "description": "rrf score: [0.5] computed for initial ranks [0, 1] with rankConstant: [1] as sum of [1 / (rank + rankConstant)] for each query",
                    "details": [
                        {
                            "value": 0,
                            "description": "rrf score: [0], result not found in query at index [0]",
                            "details": []
                        },
                        {
                            "value": 1,
                            "description": "rrf score: [0.5], for rank [1] in query at index [1] computed as [1 / (1 + 1)], for matching query with score",
                            "details": [
                                {
                                    "value": 0.8333334,
                                    "description": "rrf score: [0.8333334] computed for initial ranks [2, 1] with rankConstant: [1] as sum of [1 / (rank + rankConstant)] for each query",
                                    "details": [
                                        {
                                            "value": 2,
                                            "description": "rrf score: [0.33333334], for rank [2] in query at index [0] computed as [1 / (2 + 1)], for matching query with score",
                                            "details": [
                                                {
                                                    "value": 2.8129659,
                                                    "description": "sum of:",
                                                    "details": [
                                                        {
                                                            "value": 1.4064829,
                                                            "description": "weight(text:information in 0) [PerFieldSimilarity], result of:",
                                                            "details": [
                                                                {
                                                                    ...
                                                                }
                                                            ]
                                                        },
                                                        {
                                                            "value": 1.4064829,
                                                            "description": "weight(text:retrieval in 0) [PerFieldSimilarity], result of:",
                                                            "details": [
                                                                {
                                                                    ...
                                                                }
                                                            ]
                                                        }
                                                    ]
                                                }
                                            ]
                                        },
                                        {
                                            "value": 1,
                                            "description": "rrf score: [0.5], for rank [1] in query at index [1] computed as [1 / (1 + 1)], for matching query with score",
                                            "details": [
                                                {
                                                    "value": 1,
                                                    "description": "doc [0] with an original score of [1.0] is at rank [1] from the following source queries.",
                                                    "details": [
                                                        {
                                                            "value": 1,
                                                            "description": "found vector with calculated similarity: 1.0",
                                                            "details": []
                                                        }
                                                    ]
                                                }
                                            ]
                                        }
                                    ]
                                }
                            ]
                        }
                    ]
                }
            }
        ]
    }
}
----
// TESTRESPONSE[s/"took": 42/"took": $body.took/]
// TESTRESPONSE[s/"\.\.\."/$body.hits.hits.0._explanation.details/]

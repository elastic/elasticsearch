[[semantic-search-inference]]
=== Tutorial: semantic search with the {infer} API

++++
<titleabbrev>Semantic search with the {infer} API</titleabbrev>
++++

The instructions in this tutorial shows you how to use the {infer} API workflow with various services to perform semantic search on your data.

IMPORTANT: For the easiest way to perform semantic search in the {stack}, refer to the <<semantic-search-semantic-text, `semantic_text`>> end-to-end tutorial.

The following examples use the:

* `embed-english-v3.0` model for https://docs.cohere.com/docs/cohere-embed[Cohere]
* `all-mpnet-base-v2` model from https://huggingface.co/sentence-transformers/all-mpnet-base-v2[HuggingFace]
* `text-embedding-ada-002` second generation embedding model for OpenAI
* models available through https://ai.azure.com/explore/models?selectedTask=embeddings[Azure AI Studio] or https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/models[Azure OpenAI]
* `text-embedding-004` model for https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/text-embeddings-api[Google Vertex AI]
* `mistral-embed` model for https://docs.mistral.ai/getting-started/models/[Mistral]
* `amazon.titan-embed-text-v1` model for https://docs.aws.amazon.com/bedrock/latest/userguide/model-ids.html[Amazon Bedrock]
* `ops-text-embedding-zh-001` model for https://help.aliyun.com/zh/open-search/search-platform/developer-reference/text-embedding-api-details[AlibabaCloud AI]

You can use any Cohere and OpenAI models, they are all supported by the {infer} API.
For a list of recommended models available on HuggingFace, refer to <<inference-example-hugging-face-supported-models, the supported model list>>.

Click the name of the service you want to use on any of the widgets below to review the corresponding instructions.

[discrete]
[[infer-service-requirements]]
==== Requirements

include::{es-ref-dir}/tab-widgets/inference-api/infer-api-requirements-widget.asciidoc[]

[discrete]
[[infer-text-embedding-task]]
==== Create an inference endpoint

Create an {infer} endpoint by using the <<put-inference-api>>:

include::{es-ref-dir}/tab-widgets/inference-api/infer-api-task-widget.asciidoc[]


[discrete]
[[infer-service-mappings]]
==== Create the index mapping

The mapping of the destination index - the index that contains the embeddings that the model will create based on your input text - must be created.
The destination index must have a field with the <<dense-vector, `dense_vector`>> field type for most models and the <<sparse-vector, `sparse_vector`>> field type for the sparse vector models like in the case of the `elser` service to index the output of the used model.

include::{es-ref-dir}/tab-widgets/inference-api/infer-api-mapping-widget.asciidoc[]

[discrete]
[[infer-service-inference-ingest-pipeline]]
==== Create an ingest pipeline with an inference processor

Create an <<ingest,ingest pipeline>> with an <<inference-processor,{infer} processor>> and use the model you created above to infer against the data that is being ingested in the pipeline.

include::{es-ref-dir}/tab-widgets/inference-api/infer-api-ingest-pipeline-widget.asciidoc[]

[discrete]
[[infer-load-data]]
==== Load data

In this step, you load the data that you later use in the {infer} ingest pipeline to create embeddings from it.

Use the `msmarco-passagetest2019-top1000` data set, which is a subset of the MS MARCO Passage Ranking data set.
It consists of 200 queries, each accompanied by a list of relevant text passages.
All unique passages, along with their IDs, have been extracted from that data set and compiled into a
https://github.com/elastic/stack-docs/blob/main/docs/en/stack/ml/nlp/data/msmarco-passagetest2019-unique.tsv[tsv file].

Download the file and upload it to your cluster using the
{kibana-ref}/connect-to-elasticsearch.html#upload-data-kibana[Data Visualizer]
in the {ml-app} UI.
Assign the name `id` to the first column and `content` to the second column.
The index name is `test-data`.
Once the upload is complete, you can see an index named `test-data` with 182469 documents.

[discrete]
[[reindexing-data-infer]]
==== Ingest the data through the {infer} ingest pipeline

Create embeddings from the text by reindexing the data through the {infer} pipeline that uses your chosen model.
This step uses the {ref}/docs-reindex.html[reindex API] to simulate data ingestion through a pipeline.

include::{es-ref-dir}/tab-widgets/inference-api/infer-api-reindex-widget.asciidoc[]

The call returns a task ID to monitor the progress:

[source,console]
----
GET _tasks/<task_id>
----
// TEST[skip:TBD]

You can also cancel the reindexing process if you don't want to wait until the reindexing process is fully complete which might take hours for large data sets:

[source,console]
----
POST _tasks/<task_id>/_cancel
----
// TEST[skip:TBD]


[discrete]
[[infer-semantic-search]]
==== Semantic search

After the data set has been enriched with the embeddings, you can query the data using {ref}/knn-search.html#knn-semantic-search[semantic search].
In case of dense vector models, pass a `query_vector_builder` to the k-nearest neighbor (kNN) vector search API, and provide the query text and the model you have used to create the embeddings.
In case of a sparse vector model like ELSER, use a `sparse_vector` query, and provide the query text with the model you have used to create the embeddings.

NOTE: If you cancelled the reindexing process, you run the query only a part of the data which affects the quality of your results.

include::{es-ref-dir}/tab-widgets/inference-api/infer-api-search-widget.asciidoc[]

[discrete]
[[infer-interactive-tutorials]]
==== Interactive tutorials

You can also find tutorials in an interactive Colab notebook format using the
{es} Python client:

* https://colab.research.google.com/github/elastic/elasticsearch-labs/blob/main/notebooks/integrations/cohere/inference-cohere.ipynb[Cohere {infer} tutorial notebook]
* https://colab.research.google.com/github/elastic/elasticsearch-labs/blob/main/notebooks/search/07-inference.ipynb[OpenAI {infer} tutorial notebook]

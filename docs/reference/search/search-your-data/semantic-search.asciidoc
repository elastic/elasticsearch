[[semantic-search]]
== Semantic search

Semantic search is a search method that helps you find data based on the intent and contextual meaning of a search query, instead of a match on query terms (lexical search).

{es} provides various semantic search capabilities using {ml-docs}/ml-nlp.html[natural language processing (NLP)] and vector search.
Using an NLP model enables you to extract text embeddings out of text.
Embeddings are vectors that provide a numeric representation of a text.
Pieces of content with similar meaning have similar representations.

image::images/semantic-options.svg[Overview of semantic search workflows in {es}]

You have several options for using NLP models in the {stack}:

* use the `semantic_text` workflow (recommended)
* use the {infer} API workflow
* deploy models directly in {es}

Refer to <<using-nlp-models,this section>> to choose your workflow.

You can also store your own embeddings in {es} as vectors.
Refer to <<using-query,this section>> for guidance on which query type to use for semantic search.

At query time, {es} can use the same NLP model to convert a query into embeddings, enabling you to find documents with similar text embeddings.


[discrete]
[[using-nlp-models]]
=== Choose a semantic search workflow

[discrete]
==== `semantic_text` workflow

The simplest way to use NLP models in the {stack} is through the <<semantic-search-semantic-text, `semantic_text` workflow>>.
We recommend using this approach because it abstracts away a lot of manual work.
All you need to do is create an {infer} endpoint and an index mapping to start ingesting, embedding, and querying data.
There is no need to define model-related settings and parameters, or to create {infer} ingest pipelines.
Refer to the <<put-inference-api, Create an {infer} endpoint API>> documentation for a list of supported services.

The <<semantic-search-semantic-text, Semantic search with `semantic_text`>> tutorial shows you the process end-to-end.

[discrete]
==== {infer} API workflow

The <<semantic-search-inference, {infer} API workflow>> is more complex but offers greater control over the {infer} endpoint configuration.
You need to create an {infer} endpoint, provide various model-related settings and parameters, define an index mapping, and set up an {infer} ingest pipeline with the appropriate settings.

The <<semantic-search-inference, Semantic search with the {infer} API>> tutorial shows you the process end-to-end.

[discrete]
==== Model deployment workflow

You can also deploy NLP in {es} manually, without using an {infer} endpoint.
This is the most complex and labor intensive workflow for performing semantic search in the {stack}.
You need to select an NLP model from the {ml-docs}/ml-nlp-model-ref.html#ml-nlp-model-ref-text-embedding[list of supported dense and sparse vector models], deploy it using the Eland client, create an index mapping, and set up a suitable ingest pipeline to start ingesting and querying data.

The <<semantic-search-deployed-nlp-model, Semantic search with a model deployed in {es}>> tutorial shows you the process end-to-end.


[discrete]
[[using-query]]
=== Using the right query

Crafting the right query is crucial for semantic search.
Which query you use and which field you target in your queries depends on your chosen workflow.
If you're using the `semantic_text` workflow it's quite simple.
If not, it depends on which type of embeddings you're working with.

[cols="30%, 30%, 40%", options="header"]
|=======================================================================================================================================================================================================
| Field type to query                    | Query to use                                      | Notes                                                                                                                                                             
| <<semantic-text,`semantic_text`>>      | <<query-dsl-semantic-query,`semantic`>>           | The `semantic_text` field handles generating embeddings for you at index time and query time.                                                               
| <<sparse-vector,`sparse_vector`>>      | <<query-dsl-sparse-vector-query,`sparse_vector`>> | The `sparse_vector` query can generate query embeddings for you, but you can also provide your own. You must provide embeddings at index time.
| <<dense-vector,`dense_vector`>>        | <<query-dsl-knn-query,`knn`>>                     | The `knn` query can generate query embeddings for you, but you can also provide your own. You must provide embeddings at index time.
|=======================================================================================================================================================================================================

If you want {es} to generate embeddings at both index and query time, use the `semantic_text` field and the `semantic` query.
If you want to bring your own embeddings, use the `sparse_vector` or `dense_vector` field type and the associated query depending on the NLP model you used to generate the embeddings.

IMPORTANT: For the easiest way to perform semantic search in the {stack}, refer to the <<semantic-search-semantic-text, `semantic_text`>> end-to-end tutorial.


[discrete]
[[semantic-search-read-more]]
=== Read more

* Tutorials:
** <<semantic-search-semantic-text, Semantic search with `semantic_text`>>
** <<semantic-search-inference, Semantic search with the {infer} API>>
** <<semantic-search-elser,Semantic search with ELSER>> using the model deployment workflow
** <<semantic-search-deployed-nlp-model, Semantic search with a model deployed in {es}>>
** {ml-docs}/ml-nlp-text-emb-vector-search-example.html[Semantic search with the msmarco-MiniLM-L-12-v3 sentence-transformer model]
* Interactive examples:
** The https://github.com/elastic/elasticsearch-labs[`elasticsearch-labs`] repo contains a number of interactive semantic search examples in the form of executable Python notebooks, using the {es} Python client
** https://github.com/elastic/elasticsearch-labs/blob/main/notebooks/search/03-ELSER.ipynb[Semantic search with ELSER using the model deployment workflow]
** https://github.com/elastic/elasticsearch-labs/blob/main/notebooks/search/09-semantic-text.ipynb[Semantic search with `semantic_text`]
* Blogs:
** https://www.elastic.co/search-labs/blog/semantic-search-simplified-semantic-text[{es} new semantic_text mapping: Simplifying semantic search]
** {blog-ref}may-2023-launch-sparse-encoder-ai-model[Introducing Elastic Learned Sparse Encoder: Elastic's AI model for semantic search]
** {blog-ref}lexical-ai-powered-search-elastic-vector-database[How to get the best of lexical and AI-powered search with Elastic's vector database]
** Information retrieval blog series:
*** {blog-ref}improving-information-retrieval-elastic-stack-search-relevance[Part 1: Steps to improve search relevance]
*** {blog-ref}improving-information-retrieval-elastic-stack-benchmarking-passage-retrieval[Part 2: Benchmarking passage retrieval]
*** {blog-ref}may-2023-launch-information-retrieval-elasticsearch-ai-model[Part 3: Introducing Elastic Learned Sparse Encoder, our new retrieval model]
*** {blog-ref}improving-information-retrieval-elastic-stack-hybrid[Part 4: Hybrid retrieval]


include::semantic-search-semantic-text.asciidoc[]
include::semantic-text-hybrid-search[]
include::semantic-search-inference.asciidoc[]
include::semantic-search-elser.asciidoc[]
include::cohere-es.asciidoc[]
include::semantic-search-deploy-model.asciidoc[]
include::ingest-vectors.asciidoc[]

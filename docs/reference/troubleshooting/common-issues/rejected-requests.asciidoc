[[rejected-requests]]
=== Rejected requests

When {es} rejects a request, it stops the operation and returns an error with a
`429` response code. Rejected requests are commonly caused by:

* A <<high-cpu-usage,depleted thread pool>>. A depleted `search` or `write`
thread pool returns a `TOO_MANY_REQUESTS` error message.

* A <<circuit-breaker-errors,circuit breaker error>>.

* High <<index-modules-indexing-pressure,indexing pressure>> that exceeds the
<<memory-limits,`indexing_pressure.memory.limit`>>.

****
If you're using Elastic Cloud Hosted, then you can use AutoOps to monitor your cluster. AutoOps significantly simplifies cluster management with performance recommendations, resource utilization visibility, real-time issue detection and resolution paths. For more information, refer to https://www.elastic.co/guide/en/cloud/current/ec-autoops.html[Monitor with AutoOps].
****

[discrete]
[[check-rejected-tasks]]
==== Check rejected tasks

To check the number of rejected tasks for each thread pool, use the
<<cat-thread-pool,cat thread pool API>>. A high ratio of `rejected` to
`completed` tasks, particularly in the `search` and `write` thread pools, means
{es} regularly rejects requests.

[source,console]
----
GET /_cat/thread_pool?v=true&h=id,name,queue,active,rejected,completed
----

`write` thread pool rejections frequently appear in the erring API and
correlating log as `EsRejectedExecutionException` with either
`QueueResizingEsThreadPoolExecutor` or `queue capacity`.

These errors are often related to <<task-queue-backlog,backlogged tasks>>.

[discrete]
[[check-circuit-breakers]]
==== Check circuit breakers

To check the number of tripped <<circuit-breaker,circuit breakers>>, use the
<<cluster-nodes-stats,node stats API>>.

[source,console]
----
GET /_nodes/stats/breaker
----

These statistics are cumulative from node startup. For more information, see
<<circuit-breaker,circuit breaker errors>>.

[discrete]
[[check-indexing-pressure]]
==== Check indexing pressure

To check the number of <<index-modules-indexing-pressure,indexing pressure>>
rejections, use the <<cluster-nodes-stats,node stats API>>.

[source,console]
----
GET _nodes/stats?human&filter_path=nodes.*.indexing_pressure
----

These stats are cumulative from node startup.

Indexing pressure rejections appear as an
`EsRejectedExecutionException`, and indicate that they were rejected due
to `combined_coordinating_and_primary`, `coordinating`, `primary`, or `replica`.

These errors are often related to <<task-queue-backlog,backlogged tasks>>,
<<docs-bulk,bulk index>> sizing, or the ingest target's
<<index-modules,`refresh_interval` setting>>.

NOTE: Another cause of indexing pressure rejections might be the use of the <<semantic-text,`semantic_text`>> field type, which can cause rejections when indexing large batches of documents if the batch may otherwise incur an Out of Memory (OOM) error.

[discrete]
[[prevent-rejected-requests]]
==== Prevent rejected requests

[discrete]
[[fix-high-cpu-and-memory-usage]]
===== Fix high CPU and memory usage

If {es} regularly rejects requests and other tasks, your cluster likely has high
CPU usage or high JVM memory pressure. For tips, see <<high-cpu-usage>> and
<<high-jvm-memory-pressure>>.

[discrete]
[[fix-semantic-text-ingestion-issues]]
===== Fix for `semantic_text` ingestion issues

When bulk indexing documents with the `semantic_text` field type, you may encounter rejections due to high memory usage during inference processing.
These rejections will appear as an `InferenceException` in your cluster logs.

**To resolve this issue:**

1. Reduce the batch size of documents in your indexing requests.
2. If reducing batch size doesn't resolve the issue, then consider scaling up your machine resources.
3. A last resort option is to adjust the `indexing_pressure.memory.coordinating.limit` cluster setting. The default value is 10% of the heap. Increasing this limit allows more memory to be used for coordinating operations before rejections occur.

WARNING: This adjustment should only be considered after exhausting other options, as setting this value too high may risk Out of Memory (OOM) errors in your cluster. A cluster restart is required for this change to take effect.

---
"Custom normalizer with illegal filter in request":
    # Tests analyze api with normalizer. This is in the analysis-common module
    # because there are no filters that support multiTermAware
    - do:
        catch: bad_request
        indices.analyze:
          body:
            text: ABc
            explain: true
            filter: [word_delimiter]

    - match: { status: 400 }
    - match: { error.type: illegal_argument_exception }
    - match: { error.reason: "Custom normalizer may not use filter [word_delimiter]" }

---
"Synonym filter with tokenizer":
    - do:
        indices.create:
          index: test_synonym
          body:
            settings:
              index:
                analysis:
                  tokenizer:
                    trigram:
                      type: ngram
                      min_gram: 3
                      max_gram: 3
                  filter:
                    synonym:
                      type: synonym
                      synonyms: ["kimchy => shay"]

    - do:
        indices.analyze:
          index: test_synonym
          body:
            tokenizer: trigram
            filter: [synonym]
            text: kimchy
    - length: { tokens: 2 }
    - match:  { tokens.0.token: sha }
    - match:  { tokens.1.token: hay }

---
"Custom normalizer in request":
    - do:
        indices.analyze:
          body:
            text: ABc
            explain: true
            filter: ["lowercase"]

    - length: { detail.tokenizer.tokens: 1 }
    - length: { detail.tokenfilters.0.tokens: 1 }
    - match:  { detail.tokenizer.name: keyword }
    - match:  { detail.tokenizer.tokens.0.token: ABc }
    - match:  { detail.tokenfilters.0.name: lowercase }
    - match:  { detail.tokenfilters.0.tokens.0.token: abc }

---
"Custom analyzer is not buildable":
  - requires:
      test_runner_features: [ capabilities ]
      capabilities:
        - method: PUT
          path: /{index}
          capabilities: [ hunspell_dict_400 ]
      reason: "bugfix 'hunspell_dict_400' capability required"

  - do:
      catch: bad_request
      indices.analyze:
        body:
          text: the foxes jumping quickly
          tokenizer:
            standard
          filter:
            type: hunspell
            locale: en_US

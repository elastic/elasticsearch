/*
 * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one
 * or more contributor license agreements. Licensed under the Elastic License
 * 2.0; you may not use this file except in compliance with the Elastic License
 * 2.0.
 */

apply plugin: 'elasticsearch.internal-es-plugin'
apply plugin: 'elasticsearch.publish'

esplugin {
  name = 'esql-datasource-parquet'
  description = 'Parquet format support for ESQL external data sources'
  classname = 'org.elasticsearch.xpack.esql.datasource.parquet.ParquetDataSourcePlugin'
  extendedPlugins = ['x-pack-esql']
}

base {
  archivesName = 'esql-datasource-parquet'
}

dependencies {
  // SPI interfaces from ESQL core
  compileOnly project(path: xpackModule('esql'))
  compileOnly project(path: xpackModule('esql-core'))
  compileOnly project(path: xpackModule('core'))
  compileOnly project(':server')
  compileOnly project(xpackModule('esql:compute'))
  
  // Parquet format support - using parquet-hadoop-bundle to avoid jar hell from duplicate shaded classes
  implementation('org.apache.parquet:parquet-hadoop-bundle:1.16.0')
  
  // Hadoop dependencies - required at both compile time and runtime for Parquet operations.
  // 
  // The Hadoop Configuration class is needed because:
  // 1. ParquetFileReader has method overloads that reference Configuration in their signatures
  // 2. ParquetReadOptions.Builder() constructor creates HadoopParquetConfiguration internally,
  //    which requires the Configuration class to be present even when using non-Hadoop code paths
  // 3. parquet-hadoop-bundle includes shaded Parquet classes but not Hadoop Configuration
  implementation('org.apache.hadoop:hadoop-client-api:3.4.1')
  implementation('org.apache.hadoop:hadoop-client-runtime:3.4.1')

  testImplementation project(':test:framework')
  testImplementation(testArtifact(project(xpackModule('core'))))
}

tasks.named("dependencyLicenses").configure {
  mapping from: /lucene-.*/, to: 'lucene'
  mapping from: /parquet-.*/, to: 'parquet'
  mapping from: /hadoop-.*/, to: 'hadoop'
}

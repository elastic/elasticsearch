/*
 * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one
 * or more contributor license agreements. Licensed under the Elastic License
 * 2.0; you may not use this file except in compliance with the Elastic License
 * 2.0.
 */

plugins {
  id 'idea'
}


import org.elasticsearch.gradle.internal.util.SourceDirectoryCommandLineArgumentProvider
import org.elasticsearch.gradle.internal.AbstractDependenciesTask

import static org.elasticsearch.gradle.util.PlatformUtils.normalize

apply plugin: 'elasticsearch.internal-es-plugin'
apply plugin: 'elasticsearch.internal-cluster-test'
apply plugin: 'elasticsearch.internal-test-artifact'
apply plugin: 'elasticsearch.string-templates'
apply plugin: 'elasticsearch.publish'

esplugin {
  name = 'x-pack-esql'
  description = 'The plugin that powers ESQL for Elasticsearch'
  classname = 'org.elasticsearch.xpack.esql.plugin.EsqlPlugin'
  extendedPlugins = ['x-pack-esql-core', 'lang-painless', 'x-pack-ml']
}

base {
  archivesName = 'x-pack-esql'
}

dependencies {
  compileOnly project(path: xpackModule('core'))
  compileOnly project(':modules:lang-painless:spi')
  compileOnly project(xpackModule('esql-core'))
  compileOnly project(xpackModule('ml'))
  compileOnly project(path: xpackModule('mapper-aggregate-metric'))
  compileOnly project(path: xpackModule('downsample'))
  implementation project(xpackModule('kql'))
  implementation project('compute')
  implementation project('compute:ann')
  implementation project(':libs:dissect')
  implementation project(':libs:grok')
  implementation project(':libs:exponential-histogram')
  api "org.apache.lucene:lucene-spatial3d:${versions.lucene}"
  api "org.antlr:antlr4-runtime:${versions.antlr4}"
  api project(":libs:h3")
  api project('arrow')
  
  // Arrow dependencies (needed for Iceberg Vectorized Reader integration)
  // These must be implementation scope to be available at runtime.
  // The arrow module uses 'implementation' scope, so these are NOT transitively exposed.
  implementation('org.apache.arrow:arrow-vector:18.3.0')
  implementation('org.apache.arrow:arrow-memory-core:18.3.0')
  // Arrow memory allocator - using unsafe allocator (same as used in arrow module tests)
  // Note: Cannot use arrow-memory-netty because it requires a newer Netty version than ES provides
  // (Arrow expects canReliabilyFreeDirectBuffers() method which doesn't exist in ES's Netty)
  implementation('org.apache.arrow:arrow-memory-unsafe:18.3.0')

  // Apache Iceberg with Parquet support - using parquet-hadoop-bundle to avoid jar hell from duplicate shaded classes
  implementation("org.apache.iceberg:iceberg-core:${versions.iceberg}") {
    exclude group: 'com.github.ben-manes.caffeine', module: 'caffeine'
    // Exclude commons-codec to avoid jar hell - x-pack-core already provides commons-codec:1.15
    exclude group: 'commons-codec', module: 'commons-codec'
    // Exclude slf4j-api to avoid jar hell - x-pack-core already provides slf4j-api:2.0.6
    exclude group: 'org.slf4j', module: 'slf4j-api'
  }
  implementation("org.apache.iceberg:iceberg-aws:${versions.iceberg}") {
    // Exclude AWS SDK bundle - we'll declare individual modules explicitly
    exclude group: 'software.amazon.awssdk', module: 'bundle'
    exclude group: 'commons-codec', module: 'commons-codec'
    exclude group: 'org.slf4j', module: 'slf4j-api'
  }
  implementation("org.apache.iceberg:iceberg-parquet:${versions.iceberg}") {
    exclude group: 'org.apache.parquet', module: 'parquet-hadoop'
    exclude group: 'org.apache.parquet', module: 'parquet-column'
    exclude group: 'org.apache.parquet', module: 'parquet-avro'
    exclude group: 'org.apache.parquet', module: 'parquet-format-structures'
    exclude group: 'org.apache.parquet', module: 'parquet-common'
    exclude group: 'org.apache.parquet', module: 'parquet-encoding'
    exclude group: 'org.apache.parquet', module: 'parquet-jackson'
    exclude group: 'commons-codec', module: 'commons-codec'
    exclude group: 'org.slf4j', module: 'slf4j-api'
  }
  // Iceberg Arrow integration for vectorized data reading
  implementation("org.apache.iceberg:iceberg-arrow:${versions.iceberg}") {
    exclude group: 'org.apache.parquet', module: 'parquet-avro'
    exclude group: 'org.apache.parquet', module: 'parquet-hadoop'
    exclude group: 'org.apache.parquet', module: 'parquet-column'
    exclude group: 'org.apache.parquet', module: 'parquet-format-structures'
    exclude group: 'org.apache.parquet', module: 'parquet-common'
    exclude group: 'org.apache.parquet', module: 'parquet-encoding'
    exclude group: 'org.apache.parquet', module: 'parquet-jackson'
    exclude group: 'commons-codec', module: 'commons-codec'
    exclude group: 'org.slf4j', module: 'slf4j-api'
  }
  implementation('org.apache.parquet:parquet-hadoop-bundle:1.16.0')
  implementation('com.github.ben-manes.caffeine:caffeine:2.9.3')
  
  // Hadoop dependencies - required at both compile time and runtime for Parquet operations.
  // 
  // The Hadoop Configuration class is needed because:
  // 1. ParquetFileReader has method overloads that reference Configuration in their signatures
  // 2. ParquetReadOptions.Builder() constructor creates HadoopParquetConfiguration internally,
  //    which requires the Configuration class to be present even when using non-Hadoop code paths
  // 3. parquet-hadoop-bundle includes shaded Parquet classes but not Hadoop Configuration
  //
  // Note: We tried using a Hadoop-free approach with explicit ParquetReadOptions, but
  // ParquetReadOptions.builder() internally instantiates HadoopParquetConfiguration in its
  // constructor, making Hadoop dependency unavoidable for schema resolution.
  //
  // Production data reading path (ArrowReader) works via Iceberg's FileIO abstraction:
  // - Uses Iceberg's S3FileIO (AWS SDK v2)
  // - Uses StaticTableOperations (no HadoopCatalog)
  //
  // Both dependencies are needed as 'implementation' to ensure they're bundled with the plugin:
  // - hadoop-client-api provides the Configuration class and other interfaces
  // - hadoop-client-runtime provides the implementations
  implementation('org.apache.hadoop:hadoop-client-api:3.4.1')
  implementation('org.apache.hadoop:hadoop-client-runtime:3.4.1')
  
  // AWS SDK for S3 access - following repository-s3 pattern
  // Using explicit module declarations instead of bundle for better classloading
  implementation "software.amazon.awssdk:annotations:${versions.awsv2sdk}"
  implementation "software.amazon.awssdk:apache-client:${versions.awsv2sdk}"
  implementation "software.amazon.awssdk:url-connection-client:${versions.awsv2sdk}"
  implementation "software.amazon.awssdk:auth:${versions.awsv2sdk}"
  implementation "software.amazon.awssdk:aws-core:${versions.awsv2sdk}"
  implementation "software.amazon.awssdk:aws-xml-protocol:${versions.awsv2sdk}"
  implementation "software.amazon.awssdk:aws-json-protocol:${versions.awsv2sdk}"
  implementation "software.amazon.awssdk:http-client-spi:${versions.awsv2sdk}"
  implementation "software.amazon.awssdk:identity-spi:${versions.awsv2sdk}"
  implementation "software.amazon.awssdk:metrics-spi:${versions.awsv2sdk}"
  implementation "software.amazon.awssdk:regions:${versions.awsv2sdk}"
  implementation "software.amazon.awssdk:retries-spi:${versions.awsv2sdk}"
  implementation "software.amazon.awssdk:retries:${versions.awsv2sdk}"
  implementation "software.amazon.awssdk:s3:${versions.awsv2sdk}"
  implementation "software.amazon.awssdk:sdk-core:${versions.awsv2sdk}"
  implementation "software.amazon.awssdk:services:${versions.awsv2sdk}"
  implementation "software.amazon.awssdk:sts:${versions.awsv2sdk}"
  implementation "software.amazon.awssdk:utils:${versions.awsv2sdk}"
  implementation "software.amazon.awssdk:kms:${versions.awsv2sdk}"
  implementation "software.amazon.awssdk:glue:${versions.awsv2sdk}"
  implementation "software.amazon.awssdk:dynamodb:${versions.awsv2sdk}"
  
  // Apache HTTP client for AWS SDK (required by apache-client module)
  implementation "org.apache.httpcomponents:httpclient:${versions.httpclient}"
  
  // Jackson CSV for CSV format reader
  implementation "com.fasterxml.jackson.dataformat:jackson-dataformat-csv:${versions.jackson}"
  
  runtimeOnly "commons-codec:commons-codec:${versions.commonscodec}"
  runtimeOnly "commons-logging:commons-logging:${versions.commonslogging}"
  runtimeOnly "joda-time:joda-time:2.10.14"
  runtimeOnly "org.apache.httpcomponents:httpcore:${versions.httpcore}"
  runtimeOnly "org.apache.logging.log4j:log4j-1.2-api:${versions.log4j}"
  runtimeOnly "org.reactivestreams:reactive-streams:${versions.reactive_streams}"
  runtimeOnly "org.slf4j:slf4j-api:${versions.slf4j}"
  runtimeOnly "org.apache.logging.log4j:log4j-slf4j2-impl:${versions.log4j}"
  runtimeOnly "software.amazon.awssdk:arns:${versions.awsv2sdk}"
  runtimeOnly "software.amazon.awssdk:aws-query-protocol:${versions.awsv2sdk}"
  runtimeOnly "software.amazon.awssdk:checksums-spi:${versions.awsv2sdk}"
  runtimeOnly "software.amazon.awssdk:checksums:${versions.awsv2sdk}"
  runtimeOnly "software.amazon.awssdk:endpoints-spi:${versions.awsv2sdk}"
  runtimeOnly "software.amazon.awssdk:http-auth:${versions.awsv2sdk}"
  runtimeOnly "software.amazon.awssdk:http-auth-aws:${versions.awsv2sdk}"
  runtimeOnly "software.amazon.awssdk:http-auth-spi:${versions.awsv2sdk}"
  runtimeOnly "software.amazon.awssdk:json-utils:${versions.awsv2sdk}"
  runtimeOnly "software.amazon.awssdk:profiles:${versions.awsv2sdk}"
  runtimeOnly "software.amazon.awssdk:protocol-core:${versions.awsv2sdk}"
  runtimeOnly "software.amazon.awssdk:third-party-jackson-core:${versions.awsv2sdk}"

  // Also contains a dummy processor to allow compilation with unused annotations.
  annotationProcessor project('compute:gen')

  testImplementation(project('qa:testFixtures')) {
    exclude(group: "org.elasticsearch.plugin", module: "esql")
  }
  testImplementation project(':test:framework')
  testImplementation project(':test:fixtures:s3-fixture')
  testImplementation project(':test:fixtures:aws-fixture-utils')
  testImplementation(testArtifact(project(xpackModule('core'))))
  testImplementation(testArtifact(project(xpackModule('inference'))))
  testImplementation project(path: xpackModule('enrich'))
  testImplementation project(path: xpackModule('spatial'))
  testImplementation project(path: xpackModule('inference'))
  testImplementation project(path: xpackModule('kql'))
  testImplementation project(path: xpackModule('mapper-unsigned-long'))

  testImplementation project(path: ':modules:reindex')
  testImplementation project(path: ':modules:parent-join')
  testImplementation project(path: ':modules:analysis-common')
  testImplementation project(path: ':modules:ingest-common')
  testImplementation project(path: ':modules:legacy-geo')
  testImplementation project(path: ':modules:data-streams')
  testImplementation project(path: ':modules:mapper-extras')
  testImplementation project(xpackModule('esql:compute:test'))
  testImplementation('net.nextencia:rrdiagram:0.9.4')
  testImplementation('org.webjars.npm:fontsource__roboto-mono:4.5.7')
  
  // Note: Hadoop dependencies are already provided by 'implementation' scope above
  // Tests for data generation utilities (ParquetWriterUtil, IcebergMetadataGenerator) 
  // use LocalInputFile/LocalOutputFile which bypass Hadoop FileSystem

  internalClusterTestImplementation project(":modules:mapper-extras")
  internalClusterTestImplementation project(xpackModule('inference:qa:test-service-plugin'))
  internalClusterTestImplementation(testArtifact(project(xpackModule('inference'))))
}

tasks.named('splitPackagesAudit').configure {
  // TODO: Remove these exclusions by renaming the esql-core packages
  ignoreClasses(
    "org.elasticsearch.xpack.esql.core.util.*",
    "org.elasticsearch.xpack.esql.core.plugin.AbstractTransportQlAsyncGetResultsAction",
    "org.elasticsearch.xpack.esql.core.package-info"
  )
}

tasks.named("dependencyLicenses").configure {
  mapping from: /lucene-.*/, to: 'lucene'
  mapping from: /iceberg-.*/, to: 'iceberg'
  mapping from: /parquet-.*/, to: 'parquet'
  mapping from: /jackson-.*/, to: 'jackson'
}

tasks.withType(AbstractDependenciesTask).configureEach {
  // AWS SDK module mappings
  mapping from: 'annotations',              to: 'aws-sdk-2'
  mapping from: 'apache-client',            to: 'aws-sdk-2'
  mapping from: 'arns',                     to: 'aws-sdk-2'
  mapping from: 'auth',                     to: 'aws-sdk-2'
  mapping from: 'aws-core',                 to: 'aws-sdk-2'
  mapping from: 'aws-json-protocol',        to: 'aws-sdk-2'
  mapping from: 'aws-query-protocol',       to: 'aws-sdk-2'
  mapping from: 'aws-xml-protocol',         to: 'aws-sdk-2'
  mapping from: 'checksums',                to: 'aws-sdk-2'
  mapping from: 'checksums-spi',            to: 'aws-sdk-2'
  mapping from: 'dynamodb',                 to: 'aws-sdk-2'
  mapping from: 'endpoints-spi',            to: 'aws-sdk-2'
  mapping from: 'glue',                     to: 'aws-sdk-2'
  mapping from: 'http-auth',                to: 'aws-sdk-2'
  mapping from: 'http-auth-aws',            to: 'aws-sdk-2'
  mapping from: 'http-auth-spi',            to: 'aws-sdk-2'
  mapping from: 'http-client-spi',          to: 'aws-sdk-2'
  mapping from: 'identity-spi',             to: 'aws-sdk-2'
  mapping from: 'json-utils',               to: 'aws-sdk-2'
  mapping from: 'kms',                      to: 'aws-sdk-2'
  mapping from: 'metrics-spi',              to: 'aws-sdk-2'
  mapping from: 'profiles',                 to: 'aws-sdk-2'
  mapping from: 'protocol-core',            to: 'aws-sdk-2'
  mapping from: 'regions',                  to: 'aws-sdk-2'
  mapping from: 'retries',                  to: 'aws-sdk-2'
  mapping from: 'retries-spi',              to: 'aws-sdk-2'
  mapping from: 's3',                       to: 'aws-sdk-2'
  mapping from: 'sdk-core',                 to: 'aws-sdk-2'
  mapping from: 'services',                 to: 'aws-sdk-2'
  mapping from: 'sts',                      to: 'aws-sdk-2'
  mapping from: 'third-party-jackson-core', to: 'aws-sdk-2'
  mapping from: 'url-connection-client',    to: 'aws-sdk-2'
  mapping from: 'utils',                    to: 'aws-sdk-2'
}

tasks.named("thirdPartyAudit").configure {
  ignoreMissingClasses(
    // We use the Apache HTTP client rather than AWS CRT, so these classes are not needed
    'software.amazon.awssdk.crt.CRT',
    'software.amazon.awssdk.crt.auth.credentials.Credentials',
    'software.amazon.awssdk.crt.auth.credentials.CredentialsProvider',
    'software.amazon.awssdk.crt.auth.credentials.DelegateCredentialsProvider$DelegateCredentialsProviderBuilder',
    'software.amazon.awssdk.crt.auth.signing.AwsSigner',
    'software.amazon.awssdk.crt.auth.signing.AwsSigningConfig$AwsSignatureType',
    'software.amazon.awssdk.crt.auth.signing.AwsSigningConfig$AwsSignedBodyHeaderType',
    'software.amazon.awssdk.crt.auth.signing.AwsSigningConfig$AwsSigningAlgorithm',
    'software.amazon.awssdk.crt.auth.signing.AwsSigningConfig',
    'software.amazon.awssdk.crt.auth.signing.AwsSigningResult',
    'software.amazon.awssdk.crt.http.HttpHeader',
    'software.amazon.awssdk.crt.http.HttpMonitoringOptions',
    'software.amazon.awssdk.crt.http.HttpProxyEnvironmentVariableSetting$HttpProxyEnvironmentVariableType',
    'software.amazon.awssdk.crt.http.HttpProxyEnvironmentVariableSetting',
    'software.amazon.awssdk.crt.http.HttpProxyOptions',
    'software.amazon.awssdk.crt.http.HttpRequest',
    'software.amazon.awssdk.crt.http.HttpRequestBodyStream',
    'software.amazon.awssdk.crt.io.ClientBootstrap',
    'software.amazon.awssdk.crt.io.ExponentialBackoffRetryOptions',
    'software.amazon.awssdk.crt.io.StandardRetryOptions',
    'software.amazon.awssdk.crt.io.TlsCipherPreference',
    'software.amazon.awssdk.crt.io.TlsContext',
    'software.amazon.awssdk.crt.io.TlsContextOptions',
    'software.amazon.awssdk.crt.s3.ChecksumAlgorithm',
    'software.amazon.awssdk.crt.s3.ChecksumConfig$ChecksumLocation',
    'software.amazon.awssdk.crt.s3.ChecksumConfig',
    'software.amazon.awssdk.crt.s3.ResumeToken',
    'software.amazon.awssdk.crt.s3.S3Client',
    'software.amazon.awssdk.crt.s3.S3ClientOptions',
    'software.amazon.awssdk.crt.s3.S3FinishedResponseContext',
    'software.amazon.awssdk.crt.s3.S3MetaRequest',
    'software.amazon.awssdk.crt.s3.S3MetaRequestOptions$MetaRequestType',
    'software.amazon.awssdk.crt.s3.S3MetaRequestOptions',
    'software.amazon.awssdk.crt.s3.S3MetaRequestProgress',
    'software.amazon.awssdk.crt.s3.S3MetaRequestResponseHandler',
    'software.amazon.awssdk.crtcore.CrtConfigurationUtils',
    'software.amazon.awssdk.crtcore.CrtConnectionHealthConfiguration$Builder',
    'software.amazon.awssdk.crtcore.CrtConnectionHealthConfiguration$DefaultBuilder',
    'software.amazon.awssdk.crtcore.CrtConnectionHealthConfiguration',
    'software.amazon.awssdk.crtcore.CrtProxyConfiguration$Builder',
    'software.amazon.awssdk.crtcore.CrtProxyConfiguration$DefaultBuilder',
    'software.amazon.awssdk.crtcore.CrtProxyConfiguration',
    
    // We don't use eventstream-based features
    'software.amazon.eventstream.HeaderValue',
    'software.amazon.eventstream.Message',
    'software.amazon.eventstream.MessageDecoder'
  )
}

def generatedPath = "src/main/generated"
def projectDirectory = project.layout.projectDirectory
def generatedSourceDir = projectDirectory.dir(generatedPath)
tasks.named("compileJava").configure {
  options.compilerArgumentProviders.add(new SourceDirectoryCommandLineArgumentProvider(generatedSourceDir))
  // IntelliJ sticks generated files here and we can't stop it....
  exclude { normalize(it.file.toString()).contains("src/main/generated-src/generated") }
}

idea.module {
  sourceDirs += file(generatedPath)
}

interface Injected {
  @Inject
  FileSystemOperations getFs()
}

tasks.named("test").configure {
  // prevent empty stacktraces on JDK 25 for intrinsic Math.*Exact invocations
  // https://bugs.openjdk.org/browse/JDK-8367990
  // https://github.com/elastic/elasticsearch/issues/135009
  jvmArgs '-XX:-OmitStackTraceInFastThrow'

  ['esql', 'promql'].forEach {folder ->
    def injected = project.objects.newInstance(Injected)
    // Define the folder to delete and recreate
    def tempDir = file("build/testrun/test/temp/${folder}")
    def commandsExamplesFile = new File(tempDir, "commands.examples")
    // Find all matching .md files for commands examples
    def mdFiles = fileTree("${rootDir}/docs/reference/query-languages/${folder}/_snippets/commands/examples/") {
      include("**/*.csv-spec/*.md")
    }
    def docFolder = file("${rootDir}/docs/reference/query-languages/${folder}").toPath()
    def imagesDocFolder = file("${docFolder}/images")
    def snippetsDocFolder = file("${docFolder}/_snippets")
    def kibanaDocFolder = file("${docFolder}/kibana")
    File imagesFolder = file("build/testrun/test/temp/${folder}/images")
    File snippetsFolder = file("build/testrun/test/temp/${folder}/_snippets")
    File kibanaFolder = file("build/testrun/test/temp/${folder}/kibana")

    // BuildParams isn't available inside doFirst with --configuration-cache
    def buildParams = buildParams
    doFirst {
      injected.fs.delete {
        it.delete(tempDir)
      }
      // Re-create this folder so we can save a table of generated examples to extract from csv-spec tests
      tempDir.mkdirs() // Recreate the folder

      // Write directory name and filename of each .md file to the output file
      commandsExamplesFile.withWriter { writer ->
        mdFiles.each { file ->
          writer.writeLine("${file.parentFile.name}/${file.name}")
        }
      }
      println "File 'commands.examples' created with ${mdFiles.size()} example specifications from csv-spec files."
      if (buildParams.ci) {
        injected.fs.sync {
          from snippetsDocFolder
          into snippetsFolder
        }
        injected.fs.sync {
          from imagesDocFolder
          into imagesFolder
        }
        injected.fs.sync {
          from kibanaDocFolder
          into kibanaFolder
        }
      }
    }
    if (buildParams.ci) {
      systemProperty 'generateDocs', 'assert'
    } else {
      systemProperty 'generateDocs', 'write'
      def snippetsTree = fileTree(snippetsFolder).matching {
        include "**/types/*.md"  // Recursively include all types/*.md files (effectively counting functions and operators)
      }
      def settingsTree = fileTree(snippetsFolder).matching {
        include "**/settings/*.md"  // Recursively include all settings/*.md files
      }
      def commandsExamplesTree = fileTree(snippetsFolder).matching {
        include "**/*.csv-spec/*.md"  // Recursively include all generated *.csv-spec/*.md files (created by CommandDocsTests)
      }
      def imagesTree = fileTree(imagesFolder).matching {
        include "**/*.svg"  // Recursively include all SVG files
      }
      def kibanaTree = fileTree(kibanaFolder).matching {
        include "**/*.json"  // Recursively include all JSON files
      }

      doLast {
        def snippets = snippetsTree.files.collect { it.name }
        int countSnippets = snippets.size()
        def querySettings = settingsTree.files.collect { it.name }
        int countQuerySettings = querySettings.size()
        def commandsExamples = commandsExamplesTree.files.collect { it.name }
        int countCommandsExamples = commandsExamples.size()
        if (countSnippets == 0 && countCommandsExamples == 0 && countQuerySettings == 0) {
          logger.quiet("${folder.toUpperCase()} Docs: No function/operator snippets created. Skipping sync.")
        } else {
          logger.quiet("${folder.toUpperCase()} Docs: Found $countSnippets generated function/operator snippets and $countCommandsExamples command examples to patch into docs")
          injected.fs.sync {
            from snippetsFolder
            into snippetsDocFolder
            include '**/*.md'
            if (countSnippets <= 100) {
              // If we do not run the full test of tests, do not attempt to remove potentially unused files
              preserve {
                // The snippets directory contains generated and static content, so we must preserve all MD files.
                include '**/*.md'
              }
            } else {
              // If we do run the full test of tests, be careful about what we need to preserve
              preserve {
                // The lists and commands are static, and the operators are a mix of generated and static content
                include '*.md', '**/operators/*.md', '**/operators/**/*.md', '**/lists/*.md', '**/commands/**/*.md', '**/common/**/*.md'
              }
            }
          }
        }

        List images = imagesTree.files.collect { it.name }
        int countImages = images.size()
        Closure replaceFont = line -> {
          // The es-docs team has a recommended set of fonts for use with code, and they size similarly to the previous Roboto Mono, which is no longer available in the docs webpage
          // We do not change the original SVG generator to use these because it requires the fonts to exist in the JVM running the code
          line.replaceAll(
            /font-family:\s*Roboto Mono[^;]*;/,
            'font-family: ui-monospace, SFMono-Regular, Menlo, Monaco, Consolas, "Liberation Mono", "Courier New", monospace;'
          )
        }
        if (countImages == 0) {
          logger.quiet("${folder.toUpperCase()} Docs: No function signatures created. Skipping sync.")
        } else {
          logger.quiet("${folder.toUpperCase()} Docs: Found $countImages generated SVG files to patch into docs")
          injected.fs.sync {
            from imagesFolder
            into imagesDocFolder
            include '**/*.svg'
            if (countImages <= 100) {
              // If we do not run the full test of tests, do not attempt to remove potentially unused files
              preserve {
                // Some operator files are currently static, so we must preserve them all
                include '**/*.svg'
              }
            }
            filter replaceFont
          }
        }

        List kibana = kibanaTree.files.collect { it.name }
        int countKibana = kibana.size()
        Closure replaceLinks = line -> {
          // The kibana docs are not deployed to the normal docs location, so need absolute paths for internal references
          line.replaceAll(
            /\]\(\/reference\/([^)\s]+)\.md(#\S+)?\)/,
            '](https://www.elastic.co/docs/reference/$1$2)'
          )
        }
        if (countKibana == 0) {
          logger.quiet("${folder.toUpperCase()} Docs: No function/operator kibana docs created. Skipping sync.")
        } else {
          logger.quiet("${folder.toUpperCase()} Docs: Found $countKibana generated kibana markdown files to patch into docs")
          injected.fs.sync {
            from kibanaFolder
            into kibanaDocFolder
            include '**/*.md', '**/*.json'
            if (countKibana <= 100) {
              // If we do not run the full test of tests, do not attempt to remove potentially unused files
              preserve {
                include '**/*.md', '**/*.json'
              }
            }
            filter replaceLinks
          }
        }
      }
    }
  }
}

// This is similar to the test task above, but needed for the LookupJoinTypesIT which runs in the internalClusterTest task
// and generates a types table for the LOOKUP JOIN command. It is possible in future we might have move tests that do this.
tasks.named("internalClusterTest").configure {
  def injected = project.objects.newInstance(Injected)
  File snippetsFolder = file("build/testrun/internalClusterTest/temp/esql/_snippets")
  def snippetsDocFolder = file("${rootDir}/docs/reference/query-languages/esql/_snippets")
  if (buildParams.ci) {
    systemProperty 'generateDocs', 'assert'
    doFirst {
      injected.fs.sync {
        from snippetsDocFolder
        into snippetsFolder
      }
    }
  } else {
    systemProperty 'generateDocs', 'write'
    // Define the folder to delete and recreate
    def tempDir = file("build/testrun/internalClusterTest/temp/esql")
    doFirst {
      injected.fs.delete {
        it.delete(tempDir)
      }
      // Re-create this folder so we can save a table of generated examples to extract from csv-spec tests
      tempDir.mkdirs() // Recreate the folder
    }
    def snippetsTree = fileTree(snippetsFolder).matching {
      include "**/types/*.md"  // Recursively include all types/*.md files (effectively counting functions and operators)
    }

    doLast {
      def snippets = snippetsTree.files.collect { it.name }
      int countSnippets = snippets.size()
      if (countSnippets == 0) {
        logger.quiet("ESQL Docs: No function/operator snippets created. Skipping sync.")
      } else {
        logger.quiet("ESQL Docs: Found $countSnippets generated function/operator snippets to patch into docs")
        injected.fs.sync {
          from snippetsFolder
          into snippetsDocFolder
          include '**/*.md'
          preserve {
            include '**/*.md'
          }
        }
      }
    }
  }
}

/****************************************************************
 *  Enable QA/rest integration tests for snapshot builds only   *
 *  TODO: Enable for all builds upon this feature release       *
 ****************************************************************/
if (buildParams.snapshotBuild) {
  addQaCheckDependencies(project)
}

/**********************************************
 *          ESQL Parser regeneration           *
 **********************************************/

configurations {
  regenerate
}

dependencies {
  regenerate "org.antlr:antlr4:${versions.antlr4}"
}

String grammarPath = 'src/main/antlr'
String outputPath = 'src/main/java/org/elasticsearch/xpack/esql/parser'

pluginManager.withPlugin('com.diffplug.spotless') {
  spotless {
    java {
      // for some reason "${outputPath}/EsqlBaseParser*.java" does not match the same files...
      targetExclude "src/main/java/org/elasticsearch/xpack/esql/parser/EsqlBaseLexer*.java",
        "src/main/java/org/elasticsearch/xpack/esql/parser/EsqlBaseParser*.java",
        "src/main/java/org/elasticsearch/xpack/esql/parser/PromqlBaseLexer*.java",
        "src/main/java/org/elasticsearch/xpack/esql/parser/PromqlBaseParser*.java",
        "src/main/generated/**/*.java",
        "src/main/generated-src/generated/**/*.java"
      toggleOffOn('begin generated imports', 'end generated imports')
    }
  }
}

tasks.register("cleanGenerated", Delete) {
  delete fileTree(grammarPath) {
    include '*.tokens'
  }
  delete fileTree(outputPath) {
    include 'EsqlBase*.java'
    include 'PromqlBase*.java'
  }
}

tasks.register("regenLexer", JavaExec) {
  dependsOn "cleanGenerated"
  mainClass = 'org.antlr.v4.Tool'
  classpath = configurations.regenerate
  systemProperty 'file.encoding', 'UTF-8'
  systemProperty 'user.language', 'en'
  systemProperty 'user.country', 'US'
  systemProperty 'user.variant', ''
  args '-Werror',
    '-package', 'org.elasticsearch.xpack.esql.parser',
    '-listener',
    '-visitor',
    '-lib', "${file(grammarPath)}/lexer",
    '-o', outputPath,
    "${file(grammarPath)}/EsqlBaseLexer.g4"
}

tasks.register("regenParser", JavaExec) {
  dependsOn "cleanGenerated"
  dependsOn "regenLexer"
  mainClass = 'org.antlr.v4.Tool'
  classpath = configurations.regenerate
  systemProperty 'file.encoding', 'UTF-8'
  systemProperty 'user.language', 'en'
  systemProperty 'user.country', 'US'
  systemProperty 'user.variant', ''
  args '-Werror',
    '-package', 'org.elasticsearch.xpack.esql.parser',
    '-listener',
    '-visitor',
    '-lib', outputPath,
    '-lib', "${file(grammarPath)}/parser",
    '-o', outputPath,
    "${file(grammarPath)}/EsqlBaseParser.g4"
}

tasks.register("regenPromqlLexer", JavaExec) {
  dependsOn "cleanGenerated"
  mainClass = 'org.antlr.v4.Tool'
  classpath = configurations.regenerate
  systemProperty 'file.encoding', 'UTF-8'
  systemProperty 'user.language', 'en'
  systemProperty 'user.country', 'US'
  systemProperty 'user.variant', ''
  args '-Werror',
    '-package', 'org.elasticsearch.xpack.esql.parser',
    '-listener',
    '-visitor',
    '-lib', "${file(grammarPath)}",
    '-o', outputPath,
    "${file(grammarPath)}/PromqlBaseLexer.g4"
}

tasks.register("regenPromqlParser", JavaExec) {
  dependsOn "cleanGenerated"
  dependsOn "regenPromqlLexer"
  mainClass = 'org.antlr.v4.Tool'
  classpath = configurations.regenerate
  systemProperty 'file.encoding', 'UTF-8'
  systemProperty 'user.language', 'en'
  systemProperty 'user.country', 'US'
  systemProperty 'user.variant', ''
  args '-Werror',
    '-package', 'org.elasticsearch.xpack.esql.parser',
    '-listener',
    '-visitor',
    '-lib', outputPath,
    '-lib', "${file(grammarPath)}",
    '-o', outputPath,
    "${file(grammarPath)}/PromqlBaseParser.g4"
}

tasks.register("regen") {
  dependsOn "regenParser"
  dependsOn "regenPromqlParser"
  doLast {
    // moves token files to grammar directory for use with IDE's
    ant.move(file: "${outputPath}/EsqlBaseLexer.tokens", toDir: grammarPath)
    ant.move(file: "${outputPath}/EsqlBaseParser.tokens", toDir: grammarPath)
    ant.move(file: "${outputPath}/PromqlBaseLexer.tokens", toDir: grammarPath)
    ant.move(file: "${outputPath}/PromqlBaseParser.tokens", toDir: grammarPath)

    // make the generated classes package private
    ant.replaceregexp(
      match: 'public ((interface|class) \\Q(Es|Prom)qlBase(Parser|Lexer)\\E\\w+)',
      replace: '\\1',
      encoding: 'UTF-8'
    ) {
      fileset(dir: outputPath, includes: '*qlBase*.java')
    }
    // nuke timestamps/filenames in generated files
    ant.replaceregexp(
      match: '\\Q// Generated from \\E.*',
      replace: '\\/\\/ ANTLR GENERATED CODE: DO NOT EDIT',
      encoding: 'UTF-8'
    ) {
      fileset(dir: outputPath, includes: '*qlBase*.java')
    }
    // remove tabs in antlr generated files
    ant.replaceregexp(match: '\t', flags: 'g', replace: '  ', encoding: 'UTF-8') {
      fileset(dir: outputPath, includes: '*qlBase*.java')
    }
    // suppress this-escape warnings on EsqlBaseLexer
    ant.replaceregexp(
      match: 'public ((Es|Prom)qlBaseLexer)',
      replace: '@SuppressWarnings("this-escape")${line.separator}  public \\1',
      encoding: 'UTF-8'
    ) {
      fileset(dir: outputPath, includes: '*qlBaseLexer.java')
    }

    // suppress this-escape warnings on all internal EsqlBaseParser class constructores
    ant.replaceregexp(
      match: '([ ]+)public ([A-Z][a-z]+[a-z,A-Z]+\\()',
      flags: 'g',
      replace: '\\1@SuppressWarnings("this-escape")${line.separator}\\1public \\2',
      encoding: 'UTF-8'
    ) {
      fileset(dir: outputPath, includes: '*qlBaseParser.java')
    }
    // fix line endings
    ant.fixcrlf(srcdir: outputPath, eol: 'lf') {
      patternset(includes: '*qlBase*.java')
    }
  }
}

tasks.named("spotlessJava") { dependsOn "stringTemplates" }
tasks.named('checkstyleMain').configure {
  excludes = ["**/*.java.st"]
  exclude { normalize(it.file.toString()).contains("src/main/generated-src/generated") }
  exclude { normalize(it.file.toString()).contains("src/main/generated") }
}

def prop(Name, Type, type, TYPE, BYTES, Array) {
  return [
    "Name"       : Name,
    "Type"       : Type,
    "type"       : type,
    "TYPE"       : TYPE,
    "BYTES"      : BYTES,
    "Array"      : Array,

    "int"        : type == "int" ? "true" : "",
    "long"       : type == "long" ? "true" : "",
    "double"     : type == "double" ? "true" : "",
    "BytesRef"   : type == "BytesRef" ? "true" : "",
    "boolean"    : type == "boolean" ? "true" : "",
    "nanosMillis": Name == "NanosMillis" ? "true" : "",
    "millisNanos": Name == "MillisNanos" ? "true" : "",
  ]
}

tasks.named('stringTemplates').configure {
  var intProperties = prop("Int", "Int", "int", "INT", "Integer.BYTES", "IntArray")
  var longProperties = prop("Long", "Long", "long", "LONG", "Long.BYTES", "LongArray")
  var nanosMillisProperties = prop("NanosMillis", "Long", "long", "LONG", "Long.BYTES", "LongArray")
  var millisNanosProperties = prop("MillisNanos", "Long", "long", "LONG", "Long.BYTES", "LongArray")
  var doubleProperties = prop("Double", "Double", "double", "DOUBLE", "Double.BYTES", "DoubleArray")
  var bytesRefProperties = prop("BytesRef", "BytesRef", "BytesRef", "BYTES_REF", "org.apache.lucene.util.RamUsageEstimator.NUM_BYTES_OBJECT_REF", "")
  var booleanProperties = prop("Boolean", "Boolean", "boolean", "BOOLEAN", "Byte.BYTES", "BitArray")
  var expHistoProperties = prop("ExponentialHistogram", "ExponentialHistogram", "ExponentialHistogram", "EXPONENTIAL_HISTOGRAM", "", "")
  var tdigestProperties = prop("TDigest", "TDigest", "tdigest", "TDIGEST", "", "")

  File inInputFile = file("src/main/java/org/elasticsearch/xpack/esql/expression/predicate/operator/comparison/X-InEvaluator.java.st")
  template {
    it.properties = booleanProperties
    it.inputFile = inInputFile
    it.outputFile = "org/elasticsearch/xpack/esql/expression/predicate/operator/comparison/InBooleanEvaluator.java"
  }
  template {
    it.properties = intProperties
    it.inputFile = inInputFile
    it.outputFile = "org/elasticsearch/xpack/esql/expression/predicate/operator/comparison/InIntEvaluator.java"
  }
  template {
    it.properties = longProperties
    it.inputFile = inInputFile
    it.outputFile = "org/elasticsearch/xpack/esql/expression/predicate/operator/comparison/InLongEvaluator.java"
  }
  template {
    it.properties = nanosMillisProperties
    it.inputFile = inInputFile
    it.outputFile = "org/elasticsearch/xpack/esql/expression/predicate/operator/comparison/InNanosMillisEvaluator.java"
  }
  template {
    it.properties = millisNanosProperties
    it.inputFile = inInputFile
    it.outputFile = "org/elasticsearch/xpack/esql/expression/predicate/operator/comparison/InMillisNanosEvaluator.java"
  }
  template {
    it.properties = doubleProperties
    it.inputFile = inInputFile
    it.outputFile = "org/elasticsearch/xpack/esql/expression/predicate/operator/comparison/InDoubleEvaluator.java"
  }
  template {
    it.properties = bytesRefProperties
    it.inputFile = inInputFile
    it.outputFile = "org/elasticsearch/xpack/esql/expression/predicate/operator/comparison/InBytesRefEvaluator.java"
  }

  File coalesceInputFile = file("src/main/java/org/elasticsearch/xpack/esql/expression/function/scalar/nulls/X-CoalesceEvaluator.java.st")
  template {
    it.properties = booleanProperties
    it.inputFile = coalesceInputFile
    it.outputFile = "org/elasticsearch/xpack/esql/expression/function/scalar/nulls/CoalesceBooleanEvaluator.java"
  }
  template {
    it.properties = intProperties
    it.inputFile = coalesceInputFile
    it.outputFile = "org/elasticsearch/xpack/esql/expression/function/scalar/nulls/CoalesceIntEvaluator.java"
  }
  template {
    it.properties = longProperties
    it.inputFile = coalesceInputFile
    it.outputFile = "org/elasticsearch/xpack/esql/expression/function/scalar/nulls/CoalesceLongEvaluator.java"
  }
  template {
    it.properties = doubleProperties
    it.inputFile = coalesceInputFile
    it.outputFile = "org/elasticsearch/xpack/esql/expression/function/scalar/nulls/CoalesceDoubleEvaluator.java"
  }
  template {
    it.properties = bytesRefProperties
    it.inputFile = coalesceInputFile
    it.outputFile = "org/elasticsearch/xpack/esql/expression/function/scalar/nulls/CoalesceBytesRefEvaluator.java"
  }
  template {
    it.properties = expHistoProperties
    it.inputFile = coalesceInputFile
    it.outputFile = "org/elasticsearch/xpack/esql/expression/function/scalar/nulls/CoalesceExponentialHistogramEvaluator.java"
  }

  template {
    it.properties = tdigestProperties
    it.inputFile = coalesceInputFile
    it.outputFile = "org/elasticsearch/xpack/esql/expression/function/scalar/nulls/CoalesceTDigestEvaluator.java"
  }

  File roundToInput = file("src/main/java/org/elasticsearch/xpack/esql/expression/function/scalar/math/X-RoundTo.java.st")
  template {
    it.properties = intProperties
    it.inputFile = roundToInput
    it.outputFile = "org/elasticsearch/xpack/esql/expression/function/scalar/math/RoundToInt.java"
  }
  template {
    it.properties = longProperties
    it.inputFile = roundToInput
    it.outputFile = "org/elasticsearch/xpack/esql/expression/function/scalar/math/RoundToLong.java"
  }
  template {
    it.properties = doubleProperties
    it.inputFile = roundToInput
    it.outputFile = "org/elasticsearch/xpack/esql/expression/function/scalar/math/RoundToDouble.java"
  }
}

tasks.named("test").configure {
  if (System.getProperty("golden.noactual") != null || project.hasProperty("golden.noactual")) {
    systemProperty "golden.noactual", "true"
  }
  if (System.getProperty("golden.cleanactual") != null || project.hasProperty("golden.cleanactual")) {
    systemProperty "golden.cleanactual", "true"
  }
  if (System.getProperty("golden.overwrite") != null || project.hasProperty("golden.overwrite")) {
    systemProperty "golden.overwrite", "true"
  }
  systemProperty "policy.directory", file("${projectDir}").absolutePath
  systemProperty "java.security.policy", file("${projectDir}/test.policy").absolutePath
}

tasks.register("analyzePromqlQueries", JavaExec) {
  mainClass = 'org.elasticsearch.xpack.esql.optimizer.promql.PromqlCoverageAnalyzer'
  classpath = sourceSets.test.runtimeClasspath
  args project.findProperty("queriesFile") ?: "", project.findProperty("outputFile") ?: ""
}

// Task to generate Iceberg test fixtures
// Run with: ./gradlew :x-pack:plugin:esql:generateIcebergFixtures
tasks.register("generateIcebergFixtures", JavaExec) {
  description = 'Generate Iceberg test fixtures for integration tests'
  group = 'Test Setup'
  mainClass = 'org.elasticsearch.xpack.esql.iceberg.testdata.generation.GenerateIcebergFixtures'
  classpath = sourceSets.test.runtimeClasspath
  workingDir = projectDir
}
